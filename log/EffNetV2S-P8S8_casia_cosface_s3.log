GPU_ID [0]
============================================================
Overall Configurations:
{'SEED': 1337, 'INPUT_SIZE': [112, 112], 'EMBEDDING_SIZE': 512, 'GPU_ID': [0], 'DEVICE': device(type='cuda', index=0), 'MULTI_GPU': True, 'NUM_EPOCH': 1, 'BATCH_SIZE': 48, 'DATA_ROOT': './Data/casia-webface/', 'EVAL_PATH': './eval/', 'BACKBONE_NAME': 'EFFNET_V2', 'HEAD_NAME': 'CosFace', 'TARGET': ['lfw'], 'BACKBONE_RESUME_ROOT': './results/EffNetV2S-P8S8_casia_cosface_s2/Backbone_EFFNET_V2_checkpoint.pth', 'WORK_PATH': './results/EffNetV2S-P8S8_casia_cosface_s3'}
============================================================
./Data/casia-webface/train.rec ./Data/casia-webface/train.idx
header0 label [490624. 501196.]
id2range 10572
Number of Training Classes: 10572
./eval/lfw.bin
loading bin 0
loading bin 1000
loading bin 2000
loading bin 3000
loading bin 4000
loading bin 5000
loading bin 6000
loading bin 7000
loading bin 8000
loading bin 9000
loading bin 10000
loading bin 11000
torch.Size([12000, 3, 112, 112])
ver lfw
self.device_id [0]
self.device_id [0]
[INFO]: Loading pre-trained weights
[INFO]: Fine-tuning all layers...
self.device_id [0]
============================================================
efficientnet_v2_face(
  (model): EfficientNet(
    (features): Sequential(
      (0): Conv2dNormActivation(
        (0): Conv2d(3, 24, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU(inplace=True)
      )
      (1): Sequential(
        (0): FusedMBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.0, mode=row)
        )
        (1): FusedMBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(24, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(24, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.005, mode=row)
        )
      )
      (2): Sequential(
        (0): FusedMBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(24, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (1): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(96, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.01, mode=row)
        )
        (1): FusedMBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.015000000000000003, mode=row)
        )
        (2): FusedMBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.02, mode=row)
        )
        (3): FusedMBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.025, mode=row)
        )
      )
      (3): Sequential(
        (0): FusedMBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(48, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
              (1): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.030000000000000006, mode=row)
        )
        (1): FusedMBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.035, mode=row)
        )
        (2): FusedMBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.04, mode=row)
        )
        (3): FusedMBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.045, mode=row)
        )
      )
      (4): Sequential(
        (0): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=256, bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(256, 16, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(16, 256, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.05, mode=row)
        )
        (1): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.05500000000000001, mode=row)
        )
        (2): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.06000000000000001, mode=row)
        )
        (3): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.065, mode=row)
        )
        (4): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.07, mode=row)
        )
        (5): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
              (1): BatchNorm2d(512, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(32, 512, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.075, mode=row)
        )
      )
      (5): Sequential(
        (0): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(128, 768, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)
              (1): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(768, 32, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(32, 768, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.08, mode=row)
        )
        (1): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.085, mode=row)
        )
        (2): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.09, mode=row)
        )
        (3): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.095, mode=row)
        )
        (4): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.1, mode=row)
        )
        (5): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.10500000000000001, mode=row)
        )
        (6): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.11000000000000001, mode=row)
        )
        (7): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.11500000000000002, mode=row)
        )
        (8): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=960, bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(960, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.12000000000000002, mode=row)
        )
      )
      (6): Sequential(
        (0): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(160, 960, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(960, 960, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=960, bias=False)
              (1): BatchNorm2d(960, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(960, 40, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(40, 960, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(960, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.125, mode=row)
        )
        (1): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.13, mode=row)
        )
        (2): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.135, mode=row)
        )
        (3): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.14, mode=row)
        )
        (4): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.14500000000000002, mode=row)
        )
        (5): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.15, mode=row)
        )
        (6): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.155, mode=row)
        )
        (7): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.16, mode=row)
        )
        (8): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.165, mode=row)
        )
        (9): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.17, mode=row)
        )
        (10): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.175, mode=row)
        )
        (11): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.18, mode=row)
        )
        (12): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.185, mode=row)
        )
        (13): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.19, mode=row)
        )
        (14): MBConv(
          (block): Sequential(
            (0): Conv2dNormActivation(
              (0): Conv2d(256, 1536, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (1): Conv2dNormActivation(
              (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)
              (1): BatchNorm2d(1536, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (2): SqueezeExcitation(
              (avgpool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1536, 64, kernel_size=(1, 1), stride=(1, 1))
              (fc2): Conv2d(64, 1536, kernel_size=(1, 1), stride=(1, 1))
              (activation): SiLU(inplace=True)
              (scale_activation): Sigmoid()
            )
            (3): Conv2dNormActivation(
              (0): Conv2d(1536, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
            )
          )
          (stochastic_depth): StochasticDepth(p=0.195, mode=row)
        )
      )
      (7): Conv2dNormActivation(
        (0): Conv2d(256, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(1280, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=1)
    (classifier): Sequential(
      (0): BatchNorm1d(1280, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (1): Linear(in_features=1280, out_features=512, bias=True)
      (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (loss): CosFace(in_features = 512, out_features = 10572, s = 64.0, m = 0.35)
)
EFFNET_V2 Backbone Generated
============================================================
============================================================
AdamW (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.0

Parameter Group 1
    amsgrad: False
    betas: (0.9, 0.999)
    capturable: False
    differentiable: False
    eps: 1e-08
    foreach: None
    fused: None
    lr: 0.0005
    maximize: False
    weight_decay: 0.05
)
Optimizer Generated
============================================================
============================================================
./results/EffNetV2S-P8S8_casia_cosface_s2/Backbone_EFFNET_V2_checkpoint.pth
Loading Backbone Checkpoint './results/EffNetV2S-P8S8_casia_cosface_s2/Backbone_EFFNET_V2_checkpoint.pth'
============================================================
Device cuda:0
GPU_ID allotted is 0
Epoch 1 Learing rate : 0.0005 
Epoch 1 Batch 10	Speed: 74.11 samples/s	Training Loss 28.5330 (28.4377)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 20	Speed: 185.86 samples/s	Training Loss 30.1418 (29.8759)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 30	Speed: 185.49 samples/s	Training Loss 30.6403 (30.4631)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 40	Speed: 185.36 samples/s	Training Loss 31.3241 (30.8149)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 50	Speed: 184.96 samples/s	Training Loss 29.8835 (30.8574)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 60	Speed: 184.82 samples/s	Training Loss 31.1339 (31.0252)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 70	Speed: 184.84 samples/s	Training Loss 30.9518 (31.0332)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 80	Speed: 184.55 samples/s	Training Loss 31.0379 (31.0328)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 90	Speed: 184.24 samples/s	Training Loss 30.7639 (30.9647)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 100	Speed: 184.04 samples/s	Training Loss 31.3475 (30.9719)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][100]XNorm: 11.43666
[lfw][100]Accuracy-Flip: 0.75000+-0.01931
[lfw][100]Best-Threshold: 0.47700
highest_acc: [0.7500000000000001]
Epoch 1 Batch 110	Speed: 12.48 samples/s	Training Loss 31.1781 (30.9622)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 120	Speed: 182.92 samples/s	Training Loss 31.3584 (30.8422)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 130	Speed: 182.99 samples/s	Training Loss 31.4489 (31.1968)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 140	Speed: 182.82 samples/s	Training Loss 31.0413 (31.1563)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 150	Speed: 183.04 samples/s	Training Loss 32.3933 (31.6488)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 160	Speed: 183.45 samples/s	Training Loss 32.2744 (31.7903)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 170	Speed: 182.92 samples/s	Training Loss 30.8343 (31.4356)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 180	Speed: 183.12 samples/s	Training Loss 31.4458 (31.5340)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 190	Speed: 182.95 samples/s	Training Loss 31.5685 (31.3306)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 200	Speed: 182.87 samples/s	Training Loss 31.0949 (31.4795)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][200]XNorm: 8.26251
[lfw][200]Accuracy-Flip: 0.72650+-0.01941
[lfw][200]Best-Threshold: 0.69600
highest_acc: [0.7500000000000001]
Epoch 1 Batch 210	Speed: 12.41 samples/s	Training Loss 30.4939 (31.4396)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 220	Speed: 182.61 samples/s	Training Loss 32.2526 (31.7802)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 230	Speed: 182.59 samples/s	Training Loss 31.3337 (31.4222)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 240	Speed: 182.60 samples/s	Training Loss 31.7494 (31.6196)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 250	Speed: 182.52 samples/s	Training Loss 32.0810 (31.8058)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 260	Speed: 182.46 samples/s	Training Loss 31.0567 (31.6604)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 270	Speed: 182.61 samples/s	Training Loss 31.6407 (31.7360)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 280	Speed: 182.50 samples/s	Training Loss 32.3764 (31.7659)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 290	Speed: 182.58 samples/s	Training Loss 31.2939 (31.7687)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 300	Speed: 182.86 samples/s	Training Loss 31.9220 (31.8285)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][300]XNorm: 6.58938
[lfw][300]Accuracy-Flip: 0.71917+-0.01715
[lfw][300]Best-Threshold: 0.37400
highest_acc: [0.7500000000000001]
Epoch 1 Batch 310	Speed: 12.44 samples/s	Training Loss 32.4864 (31.9168)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 320	Speed: 183.32 samples/s	Training Loss 31.8927 (31.7869)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 330	Speed: 183.21 samples/s	Training Loss 32.0895 (31.6515)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 340	Speed: 183.17 samples/s	Training Loss 31.7972 (31.5811)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 350	Speed: 183.01 samples/s	Training Loss 31.5972 (31.5481)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 360	Speed: 182.77 samples/s	Training Loss 32.0238 (32.0186)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 370	Speed: 182.87 samples/s	Training Loss 32.3548 (32.1700)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 380	Speed: 182.98 samples/s	Training Loss 31.6727 (32.0777)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 390	Speed: 183.29 samples/s	Training Loss 32.1518 (32.0920)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 400	Speed: 183.08 samples/s	Training Loss 32.2190 (32.0041)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][400]XNorm: 17.01434
[lfw][400]Accuracy-Flip: 0.69933+-0.01911
[lfw][400]Best-Threshold: 1.13600
highest_acc: [0.7500000000000001]
Epoch 1 Batch 410	Speed: 12.20 samples/s	Training Loss 32.2928 (31.9285)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 420	Speed: 182.74 samples/s	Training Loss 31.9720 (32.0030)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 430	Speed: 182.69 samples/s	Training Loss 32.1886 (32.1289)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 440	Speed: 182.66 samples/s	Training Loss 32.3518 (32.1368)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 450	Speed: 182.67 samples/s	Training Loss 31.8868 (32.0238)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 460	Speed: 182.44 samples/s	Training Loss 32.2297 (32.0517)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 470	Speed: 182.48 samples/s	Training Loss 31.8279 (31.9176)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 480	Speed: 182.39 samples/s	Training Loss 32.3571 (31.9443)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 490	Speed: 182.35 samples/s	Training Loss 32.2939 (32.0510)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 500	Speed: 182.54 samples/s	Training Loss 31.7484 (31.8377)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][500]XNorm: 18.47327
[lfw][500]Accuracy-Flip: 0.71450+-0.01218
[lfw][500]Best-Threshold: 0.96000
highest_acc: [0.7500000000000001]
Epoch 1 Batch 510	Speed: 12.16 samples/s	Training Loss 31.5619 (32.0148)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 520	Speed: 182.63 samples/s	Training Loss 32.5827 (32.1442)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 530	Speed: 182.68 samples/s	Training Loss 32.2399 (32.0858)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 540	Speed: 182.73 samples/s	Training Loss 31.5484 (31.9479)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 550	Speed: 182.52 samples/s	Training Loss 32.6696 (32.1615)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 560	Speed: 182.82 samples/s	Training Loss 31.7743 (31.9623)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 570	Speed: 182.76 samples/s	Training Loss 31.6400 (31.8254)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 580	Speed: 182.61 samples/s	Training Loss 31.9815 (31.8521)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 590	Speed: 182.83 samples/s	Training Loss 32.1063 (31.7482)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 600	Speed: 182.50 samples/s	Training Loss 31.3283 (31.8378)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][600]XNorm: 5.68322
[lfw][600]Accuracy-Flip: 0.71833+-0.01658
[lfw][600]Best-Threshold: 1.21700
highest_acc: [0.7500000000000001]
Epoch 1 Batch 610	Speed: 12.38 samples/s	Training Loss 31.5970 (31.7782)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 620	Speed: 182.72 samples/s	Training Loss 31.9719 (31.8685)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 630	Speed: 182.87 samples/s	Training Loss 31.7001 (31.8092)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 640	Speed: 182.70 samples/s	Training Loss 31.2631 (31.5880)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 650	Speed: 182.46 samples/s	Training Loss 31.9734 (31.6544)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 660	Speed: 182.42 samples/s	Training Loss 31.7180 (31.5734)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 670	Speed: 182.47 samples/s	Training Loss 31.4837 (31.4593)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 680	Speed: 182.58 samples/s	Training Loss 31.4852 (31.4200)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 690	Speed: 182.51 samples/s	Training Loss 31.9423 (31.3607)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 700	Speed: 182.45 samples/s	Training Loss 30.7034 (31.3194)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][700]XNorm: 7.02315
[lfw][700]Accuracy-Flip: 0.73400+-0.02101
[lfw][700]Best-Threshold: 0.91900
highest_acc: [0.7500000000000001]
Epoch 1 Batch 710	Speed: 12.36 samples/s	Training Loss 31.8203 (31.4234)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 720	Speed: 182.56 samples/s	Training Loss 31.3253 (31.3783)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 730	Speed: 182.45 samples/s	Training Loss 31.4418 (31.4179)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 740	Speed: 182.68 samples/s	Training Loss 30.9876 (31.2348)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 750	Speed: 182.34 samples/s	Training Loss 31.9131 (31.3860)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 760	Speed: 181.33 samples/s	Training Loss 31.1036 (31.5444)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 770	Speed: 182.01 samples/s	Training Loss 30.9218 (31.6551)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 780	Speed: 182.44 samples/s	Training Loss 32.2820 (31.8129)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 790	Speed: 182.96 samples/s	Training Loss 31.7419 (31.5479)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 800	Speed: 182.93 samples/s	Training Loss 31.6361 (31.6472)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][800]XNorm: 5.38936
[lfw][800]Accuracy-Flip: 0.61933+-0.02062
[lfw][800]Best-Threshold: 0.72500
highest_acc: [0.7500000000000001]
Epoch 1 Batch 810	Speed: 12.38 samples/s	Training Loss 31.7564 (31.6143)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 820	Speed: 182.94 samples/s	Training Loss 31.4559 (31.7438)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 830	Speed: 182.41 samples/s	Training Loss 31.6413 (31.6767)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 840	Speed: 182.65 samples/s	Training Loss 31.0000 (31.5154)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 850	Speed: 182.20 samples/s	Training Loss 31.1117 (31.5476)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 860	Speed: 182.85 samples/s	Training Loss 31.2043 (31.7323)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 870	Speed: 182.77 samples/s	Training Loss 31.7338 (31.6235)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 880	Speed: 182.55 samples/s	Training Loss 31.9855 (31.8978)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 890	Speed: 182.50 samples/s	Training Loss 32.1016 (31.8338)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 900	Speed: 182.45 samples/s	Training Loss 31.4759 (31.6755)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][900]XNorm: 3.75680
[lfw][900]Accuracy-Flip: 0.67667+-0.01565
[lfw][900]Best-Threshold: 1.16900
highest_acc: [0.7500000000000001]
Epoch 1 Batch 910	Speed: 12.43 samples/s	Training Loss 31.6236 (31.9265)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 920	Speed: 182.19 samples/s	Training Loss 32.1554 (31.8883)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 930	Speed: 182.44 samples/s	Training Loss 31.7036 (31.6583)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 940	Speed: 182.22 samples/s	Training Loss 31.8337 (31.8416)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 950	Speed: 182.44 samples/s	Training Loss 32.1384 (32.1013)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 960	Speed: 182.32 samples/s	Training Loss 31.6415 (31.9553)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 970	Speed: 182.37 samples/s	Training Loss 31.9254 (32.1731)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 980	Speed: 182.59 samples/s	Training Loss 32.3545 (32.1762)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 990	Speed: 182.57 samples/s	Training Loss 31.8548 (32.0378)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1000	Speed: 182.35 samples/s	Training Loss 32.2129 (32.0076)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][1000]XNorm: 4.28992
[lfw][1000]Accuracy-Flip: 0.64150+-0.02639
[lfw][1000]Best-Threshold: 1.15200
highest_acc: [0.7500000000000001]
Epoch 1 Batch 1010	Speed: 12.18 samples/s	Training Loss 31.8827 (32.1185)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1020	Speed: 182.38 samples/s	Training Loss 31.9266 (31.9879)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1030	Speed: 182.49 samples/s	Training Loss 32.4894 (31.8310)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1040	Speed: 182.53 samples/s	Training Loss 31.8599 (31.9482)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1050	Speed: 182.42 samples/s	Training Loss 32.1567 (31.9925)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1060	Speed: 182.43 samples/s	Training Loss 31.7295 (31.9030)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1070	Speed: 182.53 samples/s	Training Loss 32.1232 (31.8250)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1080	Speed: 182.76 samples/s	Training Loss 31.4590 (31.6834)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1090	Speed: 182.57 samples/s	Training Loss 31.8715 (31.8240)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1100	Speed: 182.52 samples/s	Training Loss 32.0110 (31.8279)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][1100]XNorm: 4.91590
[lfw][1100]Accuracy-Flip: 0.64083+-0.02518
[lfw][1100]Best-Threshold: 0.88500
highest_acc: [0.7500000000000001]
Epoch 1 Batch 1110	Speed: 12.39 samples/s	Training Loss 31.6556 (32.0008)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1120	Speed: 182.73 samples/s	Training Loss 31.6319 (31.6065)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1130	Speed: 182.56 samples/s	Training Loss 32.0586 (31.7419)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1140	Speed: 182.67 samples/s	Training Loss 31.6756 (31.6564)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1150	Speed: 182.43 samples/s	Training Loss 31.7926 (31.8825)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1160	Speed: 182.62 samples/s	Training Loss 31.5983 (31.5582)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1170	Speed: 182.62 samples/s	Training Loss 31.8073 (31.6339)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1180	Speed: 182.52 samples/s	Training Loss 31.3570 (31.6829)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1190	Speed: 182.47 samples/s	Training Loss 31.8170 (31.8136)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1200	Speed: 182.54 samples/s	Training Loss 31.7799 (31.8649)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][1200]XNorm: 15.08051
[lfw][1200]Accuracy-Flip: 0.66750+-0.02977
[lfw][1200]Best-Threshold: 0.63900
highest_acc: [0.7500000000000001]
Epoch 1 Batch 1210	Speed: 12.36 samples/s	Training Loss 32.5910 (31.6914)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1220	Speed: 182.81 samples/s	Training Loss 31.6785 (31.6354)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1230	Speed: 182.60 samples/s	Training Loss 31.8725 (31.7259)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1240	Speed: 182.68 samples/s	Training Loss 30.6859 (31.4664)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1250	Speed: 182.65 samples/s	Training Loss 31.8783 (31.5413)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1260	Speed: 182.53 samples/s	Training Loss 32.0002 (31.5080)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1270	Speed: 182.67 samples/s	Training Loss 31.7880 (31.4961)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1280	Speed: 182.61 samples/s	Training Loss 31.7678 (31.4660)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1290	Speed: 182.60 samples/s	Training Loss 31.2397 (31.5054)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1300	Speed: 182.56 samples/s	Training Loss 31.6715 (31.4155)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][1300]XNorm: 7.49282
[lfw][1300]Accuracy-Flip: 0.73450+-0.02001
[lfw][1300]Best-Threshold: 0.89700
highest_acc: [0.7500000000000001]
Epoch 1 Batch 1310	Speed: 12.40 samples/s	Training Loss 30.9660 (31.3989)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1320	Speed: 182.36 samples/s	Training Loss 31.7532 (31.5460)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1330	Speed: 182.46 samples/s	Training Loss 31.6073 (31.4759)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1340	Speed: 182.38 samples/s	Training Loss 31.6785 (31.3835)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1350	Speed: 182.40 samples/s	Training Loss 31.4867 (31.3533)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1360	Speed: 182.52 samples/s	Training Loss 31.1710 (31.4208)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1370	Speed: 182.47 samples/s	Training Loss 31.2522 (31.2600)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1380	Speed: 182.53 samples/s	Training Loss 30.9705 (31.3520)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1390	Speed: 182.53 samples/s	Training Loss 30.9097 (31.2959)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1400	Speed: 182.44 samples/s	Training Loss 31.0080 (31.2962)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][1400]XNorm: 20.49737
[lfw][1400]Accuracy-Flip: 0.73367+-0.02244
[lfw][1400]Best-Threshold: 0.87100
highest_acc: [0.7500000000000001]
Epoch 1 Batch 1410	Speed: 12.38 samples/s	Training Loss 31.4446 (31.5378)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1420	Speed: 182.57 samples/s	Training Loss 30.9066 (31.2587)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1430	Speed: 182.59 samples/s	Training Loss 31.0978 (31.3083)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1440	Speed: 182.57 samples/s	Training Loss 31.5246 (31.3082)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1450	Speed: 182.60 samples/s	Training Loss 31.7331 (31.3499)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1460	Speed: 182.50 samples/s	Training Loss 31.3437 (31.3263)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1470	Speed: 182.59 samples/s	Training Loss 31.2320 (31.3573)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1480	Speed: 182.52 samples/s	Training Loss 31.3966 (31.3270)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1490	Speed: 182.50 samples/s	Training Loss 31.4132 (31.3481)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1500	Speed: 182.47 samples/s	Training Loss 31.4442 (31.2161)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][1500]XNorm: 63.64497
[lfw][1500]Accuracy-Flip: 0.73617+-0.01848
[lfw][1500]Best-Threshold: 0.71600
highest_acc: [0.7500000000000001]
Epoch 1 Batch 1510	Speed: 12.39 samples/s	Training Loss 31.0177 (31.1315)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1520	Speed: 182.64 samples/s	Training Loss 32.1037 (31.4425)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1530	Speed: 182.45 samples/s	Training Loss 31.3442 (31.6683)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1540	Speed: 182.57 samples/s	Training Loss 31.2238 (31.2933)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1550	Speed: 182.72 samples/s	Training Loss 31.3411 (31.5304)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1560	Speed: 182.59 samples/s	Training Loss 31.0564 (31.4146)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1570	Speed: 182.57 samples/s	Training Loss 31.1635 (31.4849)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1580	Speed: 182.78 samples/s	Training Loss 31.1324 (31.4427)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1590	Speed: 182.49 samples/s	Training Loss 30.9613 (31.2980)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1600	Speed: 182.51 samples/s	Training Loss 31.7357 (31.4917)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][1600]XNorm: 3.46789
[lfw][1600]Accuracy-Flip: 0.71117+-0.02159
[lfw][1600]Best-Threshold: 0.91100
highest_acc: [0.7500000000000001]
Epoch 1 Batch 1610	Speed: 12.21 samples/s	Training Loss 31.3801 (31.2335)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1620	Speed: 182.61 samples/s	Training Loss 31.6705 (31.4071)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1630	Speed: 182.64 samples/s	Training Loss 31.4536 (31.2092)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1640	Speed: 182.58 samples/s	Training Loss 31.5582 (31.3369)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1650	Speed: 182.55 samples/s	Training Loss 31.2042 (31.0511)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1660	Speed: 182.51 samples/s	Training Loss 31.3965 (31.1799)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1670	Speed: 182.44 samples/s	Training Loss 30.8365 (31.2494)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1680	Speed: 182.50 samples/s	Training Loss 31.5143 (31.2060)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1690	Speed: 182.50 samples/s	Training Loss 31.6839 (31.2804)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1700	Speed: 182.62 samples/s	Training Loss 31.0844 (31.0951)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][1700]XNorm: 3.62194
[lfw][1700]Accuracy-Flip: 0.72100+-0.01665
[lfw][1700]Best-Threshold: 0.87300
highest_acc: [0.7500000000000001]
Epoch 1 Batch 1710	Speed: 12.40 samples/s	Training Loss 30.7843 (31.0820)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1720	Speed: 182.39 samples/s	Training Loss 31.4683 (31.1516)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1730	Speed: 182.56 samples/s	Training Loss 30.9681 (31.2713)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1740	Speed: 182.49 samples/s	Training Loss 31.1502 (31.1533)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1750	Speed: 182.65 samples/s	Training Loss 31.1539 (31.0104)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1760	Speed: 182.63 samples/s	Training Loss 31.1712 (30.8633)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1770	Speed: 182.60 samples/s	Training Loss 30.9339 (31.2083)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1780	Speed: 182.55 samples/s	Training Loss 30.5812 (30.9351)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1790	Speed: 182.67 samples/s	Training Loss 31.4863 (31.0649)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1800	Speed: 182.61 samples/s	Training Loss 30.8668 (31.0530)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][1800]XNorm: 3.49533
[lfw][1800]Accuracy-Flip: 0.73683+-0.02224
[lfw][1800]Best-Threshold: 0.87900
highest_acc: [0.7500000000000001]
Epoch 1 Batch 1810	Speed: 12.37 samples/s	Training Loss 31.1750 (30.9409)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1820	Speed: 182.57 samples/s	Training Loss 30.9011 (30.9628)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1830	Speed: 182.59 samples/s	Training Loss 31.4313 (31.1349)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1840	Speed: 182.55 samples/s	Training Loss 31.2603 (30.9834)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1850	Speed: 182.62 samples/s	Training Loss 31.3056 (31.1744)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1860	Speed: 182.52 samples/s	Training Loss 30.4880 (30.9602)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1870	Speed: 182.50 samples/s	Training Loss 30.7779 (30.9772)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1880	Speed: 182.40 samples/s	Training Loss 30.2784 (30.6955)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1890	Speed: 182.55 samples/s	Training Loss 30.4778 (30.7264)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1900	Speed: 182.56 samples/s	Training Loss 30.7530 (30.8511)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][1900]XNorm: 3.50825
[lfw][1900]Accuracy-Flip: 0.73917+-0.01073
[lfw][1900]Best-Threshold: 0.71100
highest_acc: [0.7500000000000001]
Epoch 1 Batch 1910	Speed: 12.35 samples/s	Training Loss 30.3290 (30.8546)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1920	Speed: 190.37 samples/s	Training Loss 30.9479 (30.7370)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1930	Speed: 182.69 samples/s	Training Loss 31.3553 (30.8392)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1940	Speed: 182.57 samples/s	Training Loss 31.2980 (30.9560)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1950	Speed: 182.53 samples/s	Training Loss 30.2223 (30.8097)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1960	Speed: 182.36 samples/s	Training Loss 30.9395 (30.7876)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1970	Speed: 182.46 samples/s	Training Loss 31.6169 (31.0714)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1980	Speed: 182.48 samples/s	Training Loss 30.6185 (30.7994)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 1990	Speed: 182.58 samples/s	Training Loss 30.7978 (30.7976)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2000	Speed: 182.45 samples/s	Training Loss 30.8438 (30.7197)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][2000]XNorm: 4.14101
[lfw][2000]Accuracy-Flip: 0.75683+-0.02102
[lfw][2000]Best-Threshold: 0.83700
highest_acc: [0.7568333333333334]
Epoch 1 Batch 2010	Speed: 12.38 samples/s	Training Loss 30.6736 (30.8257)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2020	Speed: 182.62 samples/s	Training Loss 31.1735 (30.8257)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2030	Speed: 182.62 samples/s	Training Loss 30.4628 (30.8290)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2040	Speed: 182.59 samples/s	Training Loss 30.9667 (30.9086)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2050	Speed: 182.64 samples/s	Training Loss 31.1602 (31.1310)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2060	Speed: 182.65 samples/s	Training Loss 30.9404 (30.8684)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2070	Speed: 182.58 samples/s	Training Loss 31.7044 (31.0143)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2080	Speed: 182.52 samples/s	Training Loss 30.6162 (31.0203)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2090	Speed: 182.63 samples/s	Training Loss 31.1386 (30.9409)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2100	Speed: 182.66 samples/s	Training Loss 30.8910 (30.8036)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][2100]XNorm: 3.20590
[lfw][2100]Accuracy-Flip: 0.73767+-0.02555
[lfw][2100]Best-Threshold: 0.96300
highest_acc: [0.7568333333333334]
Epoch 1 Batch 2110	Speed: 12.38 samples/s	Training Loss 31.1061 (30.9594)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2120	Speed: 182.56 samples/s	Training Loss 30.5326 (30.9645)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2130	Speed: 182.48 samples/s	Training Loss 31.2310 (30.8749)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2140	Speed: 182.40 samples/s	Training Loss 30.6072 (30.8034)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2150	Speed: 182.53 samples/s	Training Loss 30.6151 (30.8381)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2160	Speed: 182.44 samples/s	Training Loss 30.9283 (30.7961)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2170	Speed: 182.75 samples/s	Training Loss 30.7287 (30.8017)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2180	Speed: 182.35 samples/s	Training Loss 30.4831 (30.6909)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2190	Speed: 182.65 samples/s	Training Loss 30.5053 (30.8698)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2200	Speed: 182.50 samples/s	Training Loss 30.5547 (30.8825)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][2200]XNorm: 3.43154
[lfw][2200]Accuracy-Flip: 0.75267+-0.02139
[lfw][2200]Best-Threshold: 0.97100
highest_acc: [0.7568333333333334]
Epoch 1 Batch 2210	Speed: 12.12 samples/s	Training Loss 30.6503 (30.7644)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2220	Speed: 182.39 samples/s	Training Loss 30.6676 (30.7354)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2230	Speed: 182.50 samples/s	Training Loss 31.2034 (30.8856)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2240	Speed: 182.51 samples/s	Training Loss 30.7749 (30.5612)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2250	Speed: 182.49 samples/s	Training Loss 30.9693 (30.7173)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2260	Speed: 182.54 samples/s	Training Loss 30.4008 (30.6606)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2270	Speed: 182.45 samples/s	Training Loss 30.2656 (30.6660)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2280	Speed: 182.37 samples/s	Training Loss 30.9277 (30.7691)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2290	Speed: 182.57 samples/s	Training Loss 30.1354 (30.5830)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2300	Speed: 182.70 samples/s	Training Loss 30.4643 (30.7494)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][2300]XNorm: 20.56832
[lfw][2300]Accuracy-Flip: 0.75850+-0.01929
[lfw][2300]Best-Threshold: 1.02900
highest_acc: [0.7585]
Epoch 1 Batch 2310	Speed: 12.36 samples/s	Training Loss 30.1666 (30.8075)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2320	Speed: 182.41 samples/s	Training Loss 30.9495 (31.1445)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2330	Speed: 182.53 samples/s	Training Loss 31.1391 (31.0245)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2340	Speed: 182.47 samples/s	Training Loss 30.9504 (31.0779)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2350	Speed: 182.49 samples/s	Training Loss 31.0444 (31.3006)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2360	Speed: 182.55 samples/s	Training Loss 31.3106 (31.1555)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2370	Speed: 182.56 samples/s	Training Loss 30.9049 (31.0488)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2380	Speed: 182.62 samples/s	Training Loss 30.9664 (31.2368)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2390	Speed: 182.41 samples/s	Training Loss 30.8214 (31.1137)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2400	Speed: 182.63 samples/s	Training Loss 30.7633 (31.0168)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][2400]XNorm: 2.86854
[lfw][2400]Accuracy-Flip: 0.71483+-0.02152
[lfw][2400]Best-Threshold: 0.70900
highest_acc: [0.7585]
Epoch 1 Batch 2410	Speed: 12.36 samples/s	Training Loss 31.1868 (31.1687)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2420	Speed: 182.45 samples/s	Training Loss 31.5892 (31.1115)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2430	Speed: 182.48 samples/s	Training Loss 31.9307 (31.2823)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2440	Speed: 182.35 samples/s	Training Loss 31.0564 (31.0688)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2450	Speed: 182.27 samples/s	Training Loss 31.3243 (31.0494)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2460	Speed: 182.34 samples/s	Training Loss 30.8386 (31.1144)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2470	Speed: 182.45 samples/s	Training Loss 31.4635 (30.9543)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2480	Speed: 182.49 samples/s	Training Loss 31.0841 (31.0545)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2490	Speed: 182.52 samples/s	Training Loss 31.1318 (31.0318)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2500	Speed: 182.38 samples/s	Training Loss 31.1857 (30.9968)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][2500]XNorm: 83.10199
[lfw][2500]Accuracy-Flip: 0.71683+-0.02531
[lfw][2500]Best-Threshold: 0.98800
highest_acc: [0.7585]
Epoch 1 Batch 2510	Speed: 12.36 samples/s	Training Loss 30.6020 (31.0684)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2520	Speed: 182.67 samples/s	Training Loss 30.9745 (31.0755)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2530	Speed: 182.58 samples/s	Training Loss 31.1017 (30.9473)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2540	Speed: 182.54 samples/s	Training Loss 30.5637 (30.8262)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2550	Speed: 182.44 samples/s	Training Loss 30.7439 (30.8060)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2560	Speed: 182.34 samples/s	Training Loss 30.7483 (30.8710)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2570	Speed: 182.55 samples/s	Training Loss 31.3367 (31.0330)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2580	Speed: 182.65 samples/s	Training Loss 30.6554 (30.7770)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2590	Speed: 182.44 samples/s	Training Loss 30.4923 (30.7109)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2600	Speed: 182.38 samples/s	Training Loss 30.5003 (30.6995)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][2600]XNorm: 10.80054
[lfw][2600]Accuracy-Flip: 0.74450+-0.02062
[lfw][2600]Best-Threshold: 0.75000
highest_acc: [0.7585]
Epoch 1 Batch 2610	Speed: 12.37 samples/s	Training Loss 30.6824 (30.7925)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2620	Speed: 182.31 samples/s	Training Loss 30.9620 (30.7449)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2630	Speed: 182.49 samples/s	Training Loss 31.0720 (30.7197)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2640	Speed: 182.60 samples/s	Training Loss 30.8497 (30.8377)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2650	Speed: 182.61 samples/s	Training Loss 30.6836 (30.9103)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2660	Speed: 182.49 samples/s	Training Loss 30.4498 (30.6234)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2670	Speed: 182.39 samples/s	Training Loss 30.7966 (30.6914)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2680	Speed: 182.36 samples/s	Training Loss 30.3277 (30.6062)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2690	Speed: 182.47 samples/s	Training Loss 30.7357 (30.6681)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2700	Speed: 182.37 samples/s	Training Loss 30.6600 (30.8582)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][2700]XNorm: 59.77458
[lfw][2700]Accuracy-Flip: 0.72567+-0.02436
[lfw][2700]Best-Threshold: 0.59600
highest_acc: [0.7585]
Epoch 1 Batch 2710	Speed: 12.32 samples/s	Training Loss 30.6514 (30.8205)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2720	Speed: 182.68 samples/s	Training Loss 31.0074 (30.7633)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2730	Speed: 182.46 samples/s	Training Loss 31.1838 (30.7693)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2740	Speed: 182.39 samples/s	Training Loss 30.8631 (30.8580)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2750	Speed: 182.42 samples/s	Training Loss 30.4623 (30.7186)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2760	Speed: 182.46 samples/s	Training Loss 30.5511 (30.7709)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2770	Speed: 182.53 samples/s	Training Loss 30.8828 (30.5675)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2780	Speed: 182.47 samples/s	Training Loss 30.4205 (30.7068)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2790	Speed: 182.46 samples/s	Training Loss 30.6093 (30.5074)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2800	Speed: 182.43 samples/s	Training Loss 30.7084 (30.6601)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][2800]XNorm: 3.81461
[lfw][2800]Accuracy-Flip: 0.76467+-0.01720
[lfw][2800]Best-Threshold: 0.87700
highest_acc: [0.7646666666666666]
Epoch 1 Batch 2810	Speed: 12.07 samples/s	Training Loss 30.8087 (30.7732)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2820	Speed: 182.63 samples/s	Training Loss 30.4923 (30.5786)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2830	Speed: 182.56 samples/s	Training Loss 30.4521 (30.5986)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2840	Speed: 182.45 samples/s	Training Loss 30.9614 (30.6550)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2850	Speed: 182.41 samples/s	Training Loss 30.5089 (30.5733)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2860	Speed: 182.41 samples/s	Training Loss 30.7828 (30.5070)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2870	Speed: 182.36 samples/s	Training Loss 30.5696 (30.8388)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2880	Speed: 182.20 samples/s	Training Loss 30.9287 (30.5925)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2890	Speed: 182.36 samples/s	Training Loss 30.8852 (30.6958)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2900	Speed: 182.42 samples/s	Training Loss 30.5560 (30.5752)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][2900]XNorm: 6.67951
[lfw][2900]Accuracy-Flip: 0.75417+-0.02461
[lfw][2900]Best-Threshold: 0.67100
highest_acc: [0.7646666666666666]
Epoch 1 Batch 2910	Speed: 12.35 samples/s	Training Loss 30.4419 (30.4286)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2920	Speed: 182.39 samples/s	Training Loss 30.6335 (30.5685)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2930	Speed: 182.38 samples/s	Training Loss 31.5410 (31.1448)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2940	Speed: 182.25 samples/s	Training Loss 31.2555 (31.2092)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2950	Speed: 182.30 samples/s	Training Loss 31.4515 (31.3190)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2960	Speed: 182.42 samples/s	Training Loss 31.0112 (31.1054)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2970	Speed: 182.29 samples/s	Training Loss 31.0979 (31.1053)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2980	Speed: 182.27 samples/s	Training Loss 31.2512 (31.0288)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 2990	Speed: 182.31 samples/s	Training Loss 31.4366 (31.0583)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3000	Speed: 182.14 samples/s	Training Loss 30.9553 (30.9068)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][3000]XNorm: 440.42627
[lfw][3000]Accuracy-Flip: 0.67783+-0.02391
[lfw][3000]Best-Threshold: 0.54800
highest_acc: [0.7646666666666666]
Epoch 1 Batch 3010	Speed: 12.37 samples/s	Training Loss 30.7162 (31.0272)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3020	Speed: 182.30 samples/s	Training Loss 31.4328 (31.0931)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3030	Speed: 182.39 samples/s	Training Loss 31.3660 (31.4850)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3040	Speed: 182.35 samples/s	Training Loss 30.9886 (31.4100)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3050	Speed: 182.33 samples/s	Training Loss 31.4279 (31.1959)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3060	Speed: 182.30 samples/s	Training Loss 31.8476 (31.1239)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3070	Speed: 182.24 samples/s	Training Loss 30.7817 (31.3770)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3080	Speed: 182.32 samples/s	Training Loss 31.0863 (31.2024)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3090	Speed: 182.30 samples/s	Training Loss 31.6961 (31.3051)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3100	Speed: 182.33 samples/s	Training Loss 31.1906 (31.2520)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][3100]XNorm: 2.46235
[lfw][3100]Accuracy-Flip: 0.68617+-0.01893
[lfw][3100]Best-Threshold: 0.74500
highest_acc: [0.7646666666666666]
Epoch 1 Batch 3110	Speed: 12.35 samples/s	Training Loss 31.1484 (31.0617)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3120	Speed: 182.11 samples/s	Training Loss 30.9655 (31.0555)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3130	Speed: 182.35 samples/s	Training Loss 31.3251 (31.2405)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3140	Speed: 182.44 samples/s	Training Loss 30.4430 (31.0683)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3150	Speed: 182.33 samples/s	Training Loss 31.1579 (31.0798)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3160	Speed: 182.53 samples/s	Training Loss 31.0462 (30.9481)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3170	Speed: 182.60 samples/s	Training Loss 31.4623 (31.1836)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3180	Speed: 182.36 samples/s	Training Loss 30.6350 (31.0197)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3190	Speed: 181.97 samples/s	Training Loss 30.8329 (30.9647)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3200	Speed: 181.86 samples/s	Training Loss 31.0878 (30.9076)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][3200]XNorm: 14.91453
[lfw][3200]Accuracy-Flip: 0.71817+-0.02023
[lfw][3200]Best-Threshold: 0.94300
highest_acc: [0.7646666666666666]
Epoch 1 Batch 3210	Speed: 12.34 samples/s	Training Loss 30.6644 (30.8082)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3220	Speed: 182.50 samples/s	Training Loss 31.0318 (30.9085)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3230	Speed: 182.40 samples/s	Training Loss 31.2783 (31.0375)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3240	Speed: 182.12 samples/s	Training Loss 30.5691 (30.8774)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3250	Speed: 181.69 samples/s	Training Loss 30.8759 (30.8056)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3260	Speed: 182.28 samples/s	Training Loss 30.9999 (31.1175)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3270	Speed: 182.33 samples/s	Training Loss 31.0585 (30.9910)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3280	Speed: 182.05 samples/s	Training Loss 30.8337 (31.0229)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3290	Speed: 181.81 samples/s	Training Loss 30.4881 (30.7936)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3300	Speed: 181.81 samples/s	Training Loss 31.0073 (30.9373)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][3300]XNorm: 43.15015
[lfw][3300]Accuracy-Flip: 0.70833+-0.02217
[lfw][3300]Best-Threshold: 0.92500
highest_acc: [0.7646666666666666]
Epoch 1 Batch 3310	Speed: 12.33 samples/s	Training Loss 30.5893 (30.9157)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3320	Speed: 182.54 samples/s	Training Loss 30.6532 (30.7872)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3330	Speed: 182.40 samples/s	Training Loss 30.7859 (30.7482)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3340	Speed: 182.40 samples/s	Training Loss 30.5564 (30.8831)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3350	Speed: 181.75 samples/s	Training Loss 30.8811 (30.7899)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3360	Speed: 182.06 samples/s	Training Loss 31.0883 (30.8426)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3370	Speed: 181.66 samples/s	Training Loss 31.1207 (30.8611)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3380	Speed: 181.74 samples/s	Training Loss 30.7531 (30.7812)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3390	Speed: 181.96 samples/s	Training Loss 31.3672 (30.8136)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3400	Speed: 181.79 samples/s	Training Loss 30.7898 (30.8581)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][3400]XNorm: 59.94766
[lfw][3400]Accuracy-Flip: 0.72867+-0.01678
[lfw][3400]Best-Threshold: 0.81000
highest_acc: [0.7646666666666666]
Epoch 1 Batch 3410	Speed: 12.32 samples/s	Training Loss 30.6993 (30.9687)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3420	Speed: 182.34 samples/s	Training Loss 30.4237 (30.7716)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3430	Speed: 182.20 samples/s	Training Loss 30.6321 (30.9020)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3440	Speed: 182.31 samples/s	Training Loss 30.6509 (30.6171)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3450	Speed: 182.37 samples/s	Training Loss 31.0084 (30.8120)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3460	Speed: 181.89 samples/s	Training Loss 30.2302 (30.4739)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3470	Speed: 182.02 samples/s	Training Loss 30.7582 (30.8486)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3480	Speed: 181.90 samples/s	Training Loss 30.6491 (30.7800)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3490	Speed: 181.98 samples/s	Training Loss 30.7813 (30.7725)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3500	Speed: 182.25 samples/s	Training Loss 30.8674 (30.6424)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][3500]XNorm: 19.49314
[lfw][3500]Accuracy-Flip: 0.72300+-0.01552
[lfw][3500]Best-Threshold: 0.77700
highest_acc: [0.7646666666666666]
Epoch 1 Batch 3510	Speed: 12.33 samples/s	Training Loss 31.0051 (30.8053)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3520	Speed: 182.24 samples/s	Training Loss 31.3155 (30.8454)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3530	Speed: 181.83 samples/s	Training Loss 30.3668 (30.6115)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3540	Speed: 181.93 samples/s	Training Loss 30.8592 (30.6597)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3550	Speed: 181.67 samples/s	Training Loss 31.0981 (30.7288)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3560	Speed: 181.97 samples/s	Training Loss 30.0690 (30.5768)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3570	Speed: 181.71 samples/s	Training Loss 30.1225 (30.5467)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3580	Speed: 181.75 samples/s	Training Loss 30.2912 (30.7601)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3590	Speed: 181.79 samples/s	Training Loss 30.4780 (30.5958)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3600	Speed: 181.60 samples/s	Training Loss 30.9870 (30.7480)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][3600]XNorm: 44.57449
[lfw][3600]Accuracy-Flip: 0.71917+-0.02299
[lfw][3600]Best-Threshold: 0.79200
highest_acc: [0.7646666666666666]
Epoch 1 Batch 3610	Speed: 12.34 samples/s	Training Loss 30.5003 (30.6959)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3620	Speed: 182.00 samples/s	Training Loss 30.8569 (30.6458)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3630	Speed: 182.15 samples/s	Training Loss 30.5756 (30.8948)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3640	Speed: 181.73 samples/s	Training Loss 30.7245 (30.7214)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3650	Speed: 182.04 samples/s	Training Loss 30.5114 (30.6447)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3660	Speed: 181.74 samples/s	Training Loss 30.1539 (30.6530)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3670	Speed: 181.77 samples/s	Training Loss 30.5727 (30.4997)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3680	Speed: 181.70 samples/s	Training Loss 30.5925 (30.6481)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3690	Speed: 181.82 samples/s	Training Loss 30.6309 (30.6541)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3700	Speed: 182.06 samples/s	Training Loss 31.1942 (30.7135)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][3700]XNorm: 47.05541
[lfw][3700]Accuracy-Flip: 0.73100+-0.01672
[lfw][3700]Best-Threshold: 0.94300
highest_acc: [0.7646666666666666]
Epoch 1 Batch 3710	Speed: 12.31 samples/s	Training Loss 30.7137 (30.6343)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3720	Speed: 181.98 samples/s	Training Loss 30.1701 (30.5646)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3730	Speed: 181.90 samples/s	Training Loss 30.3721 (30.4911)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3740	Speed: 181.83 samples/s	Training Loss 31.1145 (30.5091)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3750	Speed: 181.74 samples/s	Training Loss 30.7130 (30.5536)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3760	Speed: 181.72 samples/s	Training Loss 30.9665 (30.7210)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3770	Speed: 181.87 samples/s	Training Loss 30.9435 (30.6264)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3780	Speed: 181.74 samples/s	Training Loss 30.5675 (30.7292)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3790	Speed: 181.59 samples/s	Training Loss 30.9220 (30.6138)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3800	Speed: 181.96 samples/s	Training Loss 30.9219 (30.5368)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][3800]XNorm: 11.74994
[lfw][3800]Accuracy-Flip: 0.74767+-0.01401
[lfw][3800]Best-Threshold: 0.93000
highest_acc: [0.7646666666666666]
Epoch 1 Batch 3810	Speed: 12.34 samples/s	Training Loss 30.7834 (30.7005)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3820	Speed: 182.11 samples/s	Training Loss 31.0051 (30.4278)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3830	Speed: 181.87 samples/s	Training Loss 30.7708 (30.7056)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3840	Speed: 182.19 samples/s	Training Loss 30.5577 (30.6182)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3850	Speed: 182.06 samples/s	Training Loss 30.6243 (30.6765)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3860	Speed: 181.90 samples/s	Training Loss 30.6690 (30.5322)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3870	Speed: 182.08 samples/s	Training Loss 30.2760 (30.6091)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3880	Speed: 181.82 samples/s	Training Loss 30.7292 (30.6001)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3890	Speed: 182.16 samples/s	Training Loss 30.1653 (30.5423)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3900	Speed: 181.55 samples/s	Training Loss 30.4681 (30.5463)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][3900]XNorm: 8.29603
[lfw][3900]Accuracy-Flip: 0.74067+-0.01815
[lfw][3900]Best-Threshold: 0.84800
highest_acc: [0.7646666666666666]
Epoch 1 Batch 3910	Speed: 12.33 samples/s	Training Loss 30.2754 (30.5373)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3920	Speed: 182.03 samples/s	Training Loss 30.9883 (30.6635)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3930	Speed: 182.27 samples/s	Training Loss 31.0417 (30.4831)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3940	Speed: 182.16 samples/s	Training Loss 30.5323 (30.5785)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3950	Speed: 181.90 samples/s	Training Loss 30.3483 (30.6261)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3960	Speed: 181.95 samples/s	Training Loss 30.2321 (30.6513)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3970	Speed: 181.90 samples/s	Training Loss 30.9338 (30.5271)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3980	Speed: 182.06 samples/s	Training Loss 30.3101 (30.4242)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 3990	Speed: 181.97 samples/s	Training Loss 30.6631 (30.5690)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4000	Speed: 181.81 samples/s	Training Loss 30.3185 (30.4700)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][4000]XNorm: 55.33518
[lfw][4000]Accuracy-Flip: 0.74550+-0.01895
[lfw][4000]Best-Threshold: 0.79000
highest_acc: [0.7646666666666666]
Epoch 1 Batch 4010	Speed: 11.95 samples/s	Training Loss 30.5193 (30.5498)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4020	Speed: 182.54 samples/s	Training Loss 30.4762 (30.4145)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4030	Speed: 182.11 samples/s	Training Loss 30.5113 (30.5795)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4040	Speed: 182.07 samples/s	Training Loss 30.4413 (30.5876)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4050	Speed: 182.40 samples/s	Training Loss 30.1521 (30.3194)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4060	Speed: 182.39 samples/s	Training Loss 30.9113 (30.5184)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4070	Speed: 182.29 samples/s	Training Loss 30.2166 (30.3864)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4080	Speed: 181.93 samples/s	Training Loss 30.1806 (30.3607)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4090	Speed: 181.85 samples/s	Training Loss 30.1561 (30.4026)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4100	Speed: 181.89 samples/s	Training Loss 29.9489 (30.4318)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][4100]XNorm: 33.56317
[lfw][4100]Accuracy-Flip: 0.75500+-0.02045
[lfw][4100]Best-Threshold: 0.94800
highest_acc: [0.7646666666666666]
Epoch 1 Batch 4110	Speed: 12.35 samples/s	Training Loss 29.9006 (30.3414)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4120	Speed: 182.09 samples/s	Training Loss 30.0682 (30.3462)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4130	Speed: 181.98 samples/s	Training Loss 29.7863 (30.2934)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4140	Speed: 181.76 samples/s	Training Loss 31.0665 (30.5756)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4150	Speed: 182.12 samples/s	Training Loss 30.8147 (30.5456)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4160	Speed: 181.89 samples/s	Training Loss 30.4887 (30.3763)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4170	Speed: 182.08 samples/s	Training Loss 30.2468 (30.2884)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4180	Speed: 182.20 samples/s	Training Loss 30.6185 (30.4776)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4190	Speed: 181.78 samples/s	Training Loss 30.3904 (30.5871)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4200	Speed: 181.90 samples/s	Training Loss 30.3162 (30.2667)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][4200]XNorm: 22.87651
[lfw][4200]Accuracy-Flip: 0.75383+-0.02084
[lfw][4200]Best-Threshold: 0.92100
highest_acc: [0.7646666666666666]
Epoch 1 Batch 4210	Speed: 12.32 samples/s	Training Loss 30.3154 (30.4311)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4220	Speed: 182.08 samples/s	Training Loss 30.1300 (30.6136)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4230	Speed: 181.76 samples/s	Training Loss 30.7552 (30.6051)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4240	Speed: 182.18 samples/s	Training Loss 30.5017 (30.4825)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4250	Speed: 181.53 samples/s	Training Loss 30.2101 (30.3210)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4260	Speed: 181.99 samples/s	Training Loss 30.0703 (30.3455)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4270	Speed: 181.85 samples/s	Training Loss 30.1418 (30.3702)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4280	Speed: 181.69 samples/s	Training Loss 30.4487 (30.3066)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4290	Speed: 181.97 samples/s	Training Loss 30.9696 (30.4799)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4300	Speed: 181.77 samples/s	Training Loss 30.5236 (30.2804)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][4300]XNorm: 154.38613
[lfw][4300]Accuracy-Flip: 0.75217+-0.01670
[lfw][4300]Best-Threshold: 0.87100
highest_acc: [0.7646666666666666]
Epoch 1 Batch 4310	Speed: 12.34 samples/s	Training Loss 30.3559 (30.3976)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4320	Speed: 182.04 samples/s	Training Loss 30.8290 (30.4073)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4330	Speed: 181.93 samples/s	Training Loss 29.9792 (30.3626)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4340	Speed: 181.75 samples/s	Training Loss 30.6563 (30.3732)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4350	Speed: 181.78 samples/s	Training Loss 30.6486 (30.3247)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4360	Speed: 181.86 samples/s	Training Loss 30.3080 (30.3508)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4370	Speed: 181.98 samples/s	Training Loss 30.3351 (30.2612)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4380	Speed: 181.78 samples/s	Training Loss 30.2837 (30.3568)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4390	Speed: 181.83 samples/s	Training Loss 30.2487 (30.3644)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4400	Speed: 181.78 samples/s	Training Loss 29.9983 (30.3820)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][4400]XNorm: 21.47709
[lfw][4400]Accuracy-Flip: 0.75767+-0.01952
[lfw][4400]Best-Threshold: 0.78300
highest_acc: [0.7646666666666666]
Epoch 1 Batch 4410	Speed: 12.35 samples/s	Training Loss 30.3338 (30.4285)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4420	Speed: 181.76 samples/s	Training Loss 30.5958 (30.4042)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4430	Speed: 181.89 samples/s	Training Loss 30.9062 (30.3849)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4440	Speed: 182.01 samples/s	Training Loss 30.2097 (30.3208)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4450	Speed: 181.80 samples/s	Training Loss 30.9010 (30.5273)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4460	Speed: 181.81 samples/s	Training Loss 30.9108 (30.7404)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4470	Speed: 181.94 samples/s	Training Loss 31.1802 (30.7306)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4480	Speed: 181.65 samples/s	Training Loss 31.2304 (30.9896)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4490	Speed: 181.96 samples/s	Training Loss 30.9372 (30.7600)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4500	Speed: 181.69 samples/s	Training Loss 30.6870 (30.4467)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][4500]XNorm: 33.25035
[lfw][4500]Accuracy-Flip: 0.72517+-0.02657
[lfw][4500]Best-Threshold: 0.92200
highest_acc: [0.7646666666666666]
Epoch 1 Batch 4510	Speed: 12.12 samples/s	Training Loss 30.9724 (30.6182)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4520	Speed: 182.02 samples/s	Training Loss 30.7252 (30.4869)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4530	Speed: 181.65 samples/s	Training Loss 30.3776 (30.4409)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4540	Speed: 181.77 samples/s	Training Loss 30.2235 (30.4761)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4550	Speed: 181.76 samples/s	Training Loss 30.7947 (30.6712)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4560	Speed: 181.67 samples/s	Training Loss 30.8358 (30.3359)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4570	Speed: 181.94 samples/s	Training Loss 30.1774 (30.6919)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4580	Speed: 181.81 samples/s	Training Loss 31.1352 (30.9169)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4590	Speed: 181.77 samples/s	Training Loss 30.4183 (30.8619)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4600	Speed: 181.71 samples/s	Training Loss 30.8776 (30.6167)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][4600]XNorm: 4.74727
[lfw][4600]Accuracy-Flip: 0.72750+-0.02031
[lfw][4600]Best-Threshold: 0.77900
highest_acc: [0.7646666666666666]
Epoch 1 Batch 4610	Speed: 12.34 samples/s	Training Loss 31.0351 (30.7731)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4620	Speed: 181.85 samples/s	Training Loss 31.1165 (30.7970)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4630	Speed: 181.98 samples/s	Training Loss 31.0403 (30.8829)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4640	Speed: 181.78 samples/s	Training Loss 30.6395 (30.6586)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4650	Speed: 181.97 samples/s	Training Loss 30.4006 (30.4574)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4660	Speed: 181.85 samples/s	Training Loss 31.0236 (30.4480)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4670	Speed: 181.85 samples/s	Training Loss 30.6366 (30.3966)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4680	Speed: 181.92 samples/s	Training Loss 30.3016 (30.3103)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4690	Speed: 181.77 samples/s	Training Loss 30.3683 (30.3280)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4700	Speed: 181.88 samples/s	Training Loss 30.1373 (30.2542)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][4700]XNorm: 5.07692
[lfw][4700]Accuracy-Flip: 0.75433+-0.01763
[lfw][4700]Best-Threshold: 0.73400
highest_acc: [0.7646666666666666]
Epoch 1 Batch 4710	Speed: 12.34 samples/s	Training Loss 30.4190 (30.4080)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4720	Speed: 181.70 samples/s	Training Loss 30.4952 (30.3076)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4730	Speed: 181.98 samples/s	Training Loss 30.9236 (30.3282)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4740	Speed: 181.79 samples/s	Training Loss 30.2239 (30.4869)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4750	Speed: 182.11 samples/s	Training Loss 30.3659 (30.3039)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4760	Speed: 181.85 samples/s	Training Loss 31.0016 (30.5260)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4770	Speed: 181.96 samples/s	Training Loss 30.4597 (30.4227)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4780	Speed: 181.78 samples/s	Training Loss 30.8542 (30.3951)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4790	Speed: 182.09 samples/s	Training Loss 29.9369 (30.3325)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4800	Speed: 181.81 samples/s	Training Loss 30.1117 (30.3363)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][4800]XNorm: 4.86100
[lfw][4800]Accuracy-Flip: 0.75950+-0.01915
[lfw][4800]Best-Threshold: 0.93200
highest_acc: [0.7646666666666666]
Epoch 1 Batch 4810	Speed: 12.34 samples/s	Training Loss 30.6469 (30.6177)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4820	Speed: 181.80 samples/s	Training Loss 30.4460 (30.2142)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4830	Speed: 182.19 samples/s	Training Loss 30.0181 (30.4142)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4840	Speed: 181.94 samples/s	Training Loss 30.5598 (30.6516)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4850	Speed: 181.82 samples/s	Training Loss 30.2138 (30.3430)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4860	Speed: 181.74 samples/s	Training Loss 29.6210 (30.3423)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4870	Speed: 182.08 samples/s	Training Loss 29.9005 (30.2532)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4880	Speed: 181.80 samples/s	Training Loss 30.1515 (30.3411)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4890	Speed: 182.07 samples/s	Training Loss 30.2227 (30.5157)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4900	Speed: 181.69 samples/s	Training Loss 30.5781 (30.3760)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][4900]XNorm: 4.30186
[lfw][4900]Accuracy-Flip: 0.76583+-0.01965
[lfw][4900]Best-Threshold: 0.97000
highest_acc: [0.7658333333333333]
Epoch 1 Batch 4910	Speed: 12.34 samples/s	Training Loss 30.2687 (30.0433)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4920	Speed: 182.26 samples/s	Training Loss 30.3239 (30.3310)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4930	Speed: 182.08 samples/s	Training Loss 30.1264 (30.1608)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4940	Speed: 181.83 samples/s	Training Loss 30.1419 (30.1512)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4950	Speed: 182.18 samples/s	Training Loss 30.0760 (30.1912)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4960	Speed: 181.96 samples/s	Training Loss 30.0416 (29.9718)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4970	Speed: 181.90 samples/s	Training Loss 29.6994 (30.2888)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4980	Speed: 181.85 samples/s	Training Loss 30.4854 (30.3516)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 4990	Speed: 181.84 samples/s	Training Loss 30.3673 (30.3488)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5000	Speed: 181.62 samples/s	Training Loss 30.1080 (29.9945)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][5000]XNorm: 6.71242
[lfw][5000]Accuracy-Flip: 0.76567+-0.02158
[lfw][5000]Best-Threshold: 0.99700
highest_acc: [0.7658333333333333]
Epoch 1 Batch 5010	Speed: 12.32 samples/s	Training Loss 30.8507 (30.4052)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5020	Speed: 182.01 samples/s	Training Loss 30.3306 (30.4117)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5030	Speed: 181.79 samples/s	Training Loss 30.2444 (30.2351)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5040	Speed: 181.81 samples/s	Training Loss 30.5598 (30.3199)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5050	Speed: 181.90 samples/s	Training Loss 30.3509 (30.2946)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5060	Speed: 181.88 samples/s	Training Loss 30.7483 (30.4071)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5070	Speed: 181.88 samples/s	Training Loss 30.4712 (30.1769)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5080	Speed: 181.91 samples/s	Training Loss 30.0060 (30.1440)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5090	Speed: 181.79 samples/s	Training Loss 29.9495 (30.2614)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5100	Speed: 181.69 samples/s	Training Loss 30.0069 (30.3475)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][5100]XNorm: 6.58553
[lfw][5100]Accuracy-Flip: 0.76900+-0.01702
[lfw][5100]Best-Threshold: 1.06300
highest_acc: [0.769]
Epoch 1 Batch 5110	Speed: 12.14 samples/s	Training Loss 30.0574 (30.1083)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5120	Speed: 181.94 samples/s	Training Loss 30.4702 (30.3989)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5130	Speed: 182.09 samples/s	Training Loss 30.4191 (30.1627)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5140	Speed: 181.73 samples/s	Training Loss 30.1141 (30.3274)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5150	Speed: 181.91 samples/s	Training Loss 30.2456 (30.1842)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5160	Speed: 181.95 samples/s	Training Loss 30.4386 (30.3162)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5170	Speed: 181.91 samples/s	Training Loss 30.3095 (30.2932)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5180	Speed: 181.71 samples/s	Training Loss 30.0599 (30.2854)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5190	Speed: 181.73 samples/s	Training Loss 30.3880 (30.2216)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5200	Speed: 181.88 samples/s	Training Loss 30.0565 (30.1370)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][5200]XNorm: 4.03156
[lfw][5200]Accuracy-Flip: 0.76000+-0.01599
[lfw][5200]Best-Threshold: 0.83000
highest_acc: [0.769]
Epoch 1 Batch 5210	Speed: 12.35 samples/s	Training Loss 30.4205 (30.3284)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5220	Speed: 182.08 samples/s	Training Loss 30.4941 (30.2663)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5230	Speed: 181.81 samples/s	Training Loss 30.1878 (30.1865)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5240	Speed: 182.19 samples/s	Training Loss 29.5377 (30.0403)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5250	Speed: 181.84 samples/s	Training Loss 30.3396 (30.2702)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5260	Speed: 181.99 samples/s	Training Loss 30.0934 (30.2599)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5270	Speed: 181.78 samples/s	Training Loss 29.9789 (30.0792)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5280	Speed: 181.94 samples/s	Training Loss 29.8468 (30.2396)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5290	Speed: 181.89 samples/s	Training Loss 30.4875 (30.2423)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5300	Speed: 182.07 samples/s	Training Loss 30.1148 (30.2931)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][5300]XNorm: 7.41489
[lfw][5300]Accuracy-Flip: 0.76200+-0.01524
[lfw][5300]Best-Threshold: 0.97000
highest_acc: [0.769]
Epoch 1 Batch 5310	Speed: 12.34 samples/s	Training Loss 29.6664 (30.0376)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5320	Speed: 182.13 samples/s	Training Loss 30.0017 (30.2847)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5330	Speed: 182.02 samples/s	Training Loss 30.4280 (30.2065)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5340	Speed: 181.83 samples/s	Training Loss 30.3317 (30.2273)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5350	Speed: 181.88 samples/s	Training Loss 31.0123 (30.4104)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5360	Speed: 181.82 samples/s	Training Loss 30.1777 (30.3196)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5370	Speed: 181.96 samples/s	Training Loss 29.8211 (30.2234)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5380	Speed: 181.74 samples/s	Training Loss 30.0672 (30.2750)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5390	Speed: 181.80 samples/s	Training Loss 29.5391 (30.0827)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5400	Speed: 181.77 samples/s	Training Loss 29.5968 (30.1429)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][5400]XNorm: 24.37763
[lfw][5400]Accuracy-Flip: 0.75683+-0.01961
[lfw][5400]Best-Threshold: 0.92700
highest_acc: [0.769]
Epoch 1 Batch 5410	Speed: 12.34 samples/s	Training Loss 30.2729 (30.3142)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5420	Speed: 181.92 samples/s	Training Loss 30.5617 (30.1374)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5430	Speed: 181.69 samples/s	Training Loss 30.6134 (30.1746)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5440	Speed: 182.02 samples/s	Training Loss 30.0027 (30.1980)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5450	Speed: 181.78 samples/s	Training Loss 30.0951 (29.9841)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5460	Speed: 181.75 samples/s	Training Loss 30.1503 (30.1465)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5470	Speed: 181.63 samples/s	Training Loss 30.1230 (30.1193)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5480	Speed: 181.65 samples/s	Training Loss 30.1356 (30.1177)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5490	Speed: 181.89 samples/s	Training Loss 30.0721 (30.0312)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5500	Speed: 181.62 samples/s	Training Loss 30.6813 (30.0858)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][5500]XNorm: 12.30812
[lfw][5500]Accuracy-Flip: 0.77267+-0.02013
[lfw][5500]Best-Threshold: 0.92600
highest_acc: [0.7726666666666666]
Epoch 1 Batch 5510	Speed: 12.32 samples/s	Training Loss 30.2057 (30.1749)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5520	Speed: 182.02 samples/s	Training Loss 29.4603 (30.1491)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5530	Speed: 181.79 samples/s	Training Loss 30.1208 (30.1505)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5540	Speed: 181.70 samples/s	Training Loss 29.7775 (30.2258)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5550	Speed: 182.03 samples/s	Training Loss 29.7140 (30.0581)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5560	Speed: 181.75 samples/s	Training Loss 30.2208 (30.2193)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5570	Speed: 181.89 samples/s	Training Loss 29.8796 (30.1155)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5580	Speed: 181.59 samples/s	Training Loss 29.7241 (29.9631)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5590	Speed: 181.81 samples/s	Training Loss 29.7067 (29.9902)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5600	Speed: 181.74 samples/s	Training Loss 29.8822 (30.1197)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][5600]XNorm: 8.72509
[lfw][5600]Accuracy-Flip: 0.76400+-0.01886
[lfw][5600]Best-Threshold: 0.82100
highest_acc: [0.7726666666666666]
Epoch 1 Batch 5610	Speed: 12.33 samples/s	Training Loss 29.9491 (30.0537)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5620	Speed: 182.18 samples/s	Training Loss 29.6851 (30.2207)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5630	Speed: 181.85 samples/s	Training Loss 29.7940 (30.1655)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5640	Speed: 181.94 samples/s	Training Loss 30.3217 (30.1487)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5650	Speed: 181.94 samples/s	Training Loss 29.4606 (30.0505)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5660	Speed: 181.99 samples/s	Training Loss 30.4569 (30.6389)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5670	Speed: 182.01 samples/s	Training Loss 30.1104 (30.6843)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5680	Speed: 181.98 samples/s	Training Loss 30.5397 (30.5418)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5690	Speed: 181.73 samples/s	Training Loss 31.2651 (30.6376)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5700	Speed: 182.10 samples/s	Training Loss 30.6495 (30.5772)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][5700]XNorm: 5.13717
[lfw][5700]Accuracy-Flip: 0.75617+-0.02144
[lfw][5700]Best-Threshold: 0.90200
highest_acc: [0.7726666666666666]
Epoch 1 Batch 5710	Speed: 12.09 samples/s	Training Loss 29.6725 (30.4663)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5720	Speed: 182.25 samples/s	Training Loss 30.5288 (30.2163)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5730	Speed: 181.94 samples/s	Training Loss 30.9268 (30.2511)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5740	Speed: 181.97 samples/s	Training Loss 30.7965 (30.4270)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5750	Speed: 181.73 samples/s	Training Loss 30.3567 (30.2490)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5760	Speed: 181.89 samples/s	Training Loss 30.4082 (30.1286)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5770	Speed: 181.70 samples/s	Training Loss 30.6656 (30.3057)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5780	Speed: 181.78 samples/s	Training Loss 30.0407 (30.2036)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5790	Speed: 181.98 samples/s	Training Loss 29.9523 (30.0783)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5800	Speed: 182.02 samples/s	Training Loss 29.9701 (30.0675)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][5800]XNorm: 13.55850
[lfw][5800]Accuracy-Flip: 0.75933+-0.01679
[lfw][5800]Best-Threshold: 1.03800
highest_acc: [0.7726666666666666]
Epoch 1 Batch 5810	Speed: 12.29 samples/s	Training Loss 30.1271 (30.2364)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5820	Speed: 181.32 samples/s	Training Loss 29.8970 (30.2790)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5830	Speed: 181.72 samples/s	Training Loss 30.0372 (30.0609)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5840	Speed: 181.52 samples/s	Training Loss 29.3732 (30.0695)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5850	Speed: 181.67 samples/s	Training Loss 30.1042 (30.2212)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5860	Speed: 181.61 samples/s	Training Loss 29.6879 (29.9685)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5870	Speed: 181.66 samples/s	Training Loss 30.3383 (30.0897)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5880	Speed: 181.49 samples/s	Training Loss 30.0221 (30.0453)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5890	Speed: 181.63 samples/s	Training Loss 30.0771 (30.1354)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5900	Speed: 181.65 samples/s	Training Loss 30.4491 (30.2123)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][5900]XNorm: 47.00148
[lfw][5900]Accuracy-Flip: 0.76583+-0.01914
[lfw][5900]Best-Threshold: 1.00100
highest_acc: [0.7726666666666666]
Epoch 1 Batch 5910	Speed: 12.30 samples/s	Training Loss 30.0247 (29.9916)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5920	Speed: 181.56 samples/s	Training Loss 29.4195 (29.9866)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5930	Speed: 181.53 samples/s	Training Loss 29.7853 (30.1147)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5940	Speed: 181.54 samples/s	Training Loss 30.6887 (30.2652)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5950	Speed: 181.59 samples/s	Training Loss 29.6624 (29.8743)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5960	Speed: 181.51 samples/s	Training Loss 29.9692 (29.9753)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5970	Speed: 181.58 samples/s	Training Loss 29.9745 (30.1383)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5980	Speed: 181.82 samples/s	Training Loss 30.0544 (29.9257)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 5990	Speed: 181.59 samples/s	Training Loss 29.8948 (29.9845)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6000	Speed: 181.45 samples/s	Training Loss 30.2396 (30.0211)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][6000]XNorm: 37.79363
[lfw][6000]Accuracy-Flip: 0.76717+-0.02072
[lfw][6000]Best-Threshold: 0.83100
highest_acc: [0.7726666666666666]
Epoch 1 Batch 6010	Speed: 12.32 samples/s	Training Loss 30.1767 (30.0791)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6020	Speed: 181.68 samples/s	Training Loss 30.0428 (29.9653)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6030	Speed: 181.76 samples/s	Training Loss 30.2950 (30.1782)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6040	Speed: 181.66 samples/s	Training Loss 29.6681 (29.8761)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6050	Speed: 181.63 samples/s	Training Loss 29.7818 (29.9651)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6060	Speed: 181.53 samples/s	Training Loss 30.2381 (29.9022)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6070	Speed: 181.55 samples/s	Training Loss 29.9214 (30.1456)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6080	Speed: 181.68 samples/s	Training Loss 30.2899 (30.0108)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6090	Speed: 181.46 samples/s	Training Loss 30.2999 (30.0378)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6100	Speed: 181.61 samples/s	Training Loss 29.9774 (29.9364)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][6100]XNorm: 15.03863
[lfw][6100]Accuracy-Flip: 0.78017+-0.01129
[lfw][6100]Best-Threshold: 0.93500
highest_acc: [0.7801666666666668]
Epoch 1 Batch 6110	Speed: 12.31 samples/s	Training Loss 30.2590 (29.9574)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6120	Speed: 181.54 samples/s	Training Loss 30.2847 (29.9357)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6130	Speed: 181.44 samples/s	Training Loss 29.8374 (29.9612)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6140	Speed: 181.06 samples/s	Training Loss 29.8547 (29.7983)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6150	Speed: 181.87 samples/s	Training Loss 30.0114 (30.0604)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6160	Speed: 181.72 samples/s	Training Loss 29.8807 (29.8792)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6170	Speed: 181.87 samples/s	Training Loss 30.2044 (30.2091)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6180	Speed: 181.03 samples/s	Training Loss 30.2472 (29.8286)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6190	Speed: 181.27 samples/s	Training Loss 30.1517 (30.0133)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6200	Speed: 181.33 samples/s	Training Loss 30.3203 (30.1592)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][6200]XNorm: 34.95795
[lfw][6200]Accuracy-Flip: 0.77267+-0.01346
[lfw][6200]Best-Threshold: 0.89600
highest_acc: [0.7801666666666668]
Epoch 1 Batch 6210	Speed: 12.02 samples/s	Training Loss 29.6476 (29.7482)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6220	Speed: 181.42 samples/s	Training Loss 30.0605 (29.8730)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6230	Speed: 181.36 samples/s	Training Loss 30.1822 (29.8866)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6240	Speed: 181.35 samples/s	Training Loss 30.3015 (29.9800)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6250	Speed: 181.14 samples/s	Training Loss 29.9834 (29.8778)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6260	Speed: 180.79 samples/s	Training Loss 29.7119 (29.7384)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6270	Speed: 181.21 samples/s	Training Loss 30.2047 (29.8261)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6280	Speed: 181.01 samples/s	Training Loss 30.2437 (29.8882)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6290	Speed: 181.67 samples/s	Training Loss 29.9681 (30.0616)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6300	Speed: 181.36 samples/s	Training Loss 29.5517 (29.7722)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][6300]XNorm: 48.26093
[lfw][6300]Accuracy-Flip: 0.76050+-0.01400
[lfw][6300]Best-Threshold: 0.80600
highest_acc: [0.7801666666666668]
Epoch 1 Batch 6310	Speed: 12.28 samples/s	Training Loss 30.1098 (29.9996)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6320	Speed: 181.87 samples/s	Training Loss 29.5799 (29.6575)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6330	Speed: 181.63 samples/s	Training Loss 30.6910 (29.8973)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6340	Speed: 182.00 samples/s	Training Loss 29.7788 (29.7321)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6350	Speed: 181.77 samples/s	Training Loss 29.5267 (29.7760)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6360	Speed: 182.35 samples/s	Training Loss 29.8510 (29.7927)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6370	Speed: 181.91 samples/s	Training Loss 30.1072 (29.7910)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6380	Speed: 181.87 samples/s	Training Loss 30.3914 (29.7338)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6390	Speed: 181.87 samples/s	Training Loss 30.3330 (29.9323)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6400	Speed: 182.14 samples/s	Training Loss 30.0031 (29.5807)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][6400]XNorm: 29.36951
[lfw][6400]Accuracy-Flip: 0.76767+-0.01823
[lfw][6400]Best-Threshold: 0.80700
highest_acc: [0.7801666666666668]
Epoch 1 Batch 6410	Speed: 12.36 samples/s	Training Loss 29.9036 (30.0294)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6420	Speed: 181.78 samples/s	Training Loss 29.5697 (29.9656)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6430	Speed: 182.10 samples/s	Training Loss 29.9996 (29.6391)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6440	Speed: 182.25 samples/s	Training Loss 29.7964 (29.8518)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6450	Speed: 182.28 samples/s	Training Loss 30.1917 (29.8006)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6460	Speed: 181.88 samples/s	Training Loss 30.3695 (29.7686)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6470	Speed: 182.43 samples/s	Training Loss 29.3534 (29.6831)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6480	Speed: 182.14 samples/s	Training Loss 28.9896 (29.7742)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6490	Speed: 182.25 samples/s	Training Loss 29.4979 (29.7036)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6500	Speed: 182.12 samples/s	Training Loss 29.4085 (29.7761)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][6500]XNorm: 19.08163
[lfw][6500]Accuracy-Flip: 0.78583+-0.01644
[lfw][6500]Best-Threshold: 0.84700
highest_acc: [0.7858333333333334]
Epoch 1 Batch 6510	Speed: 12.35 samples/s	Training Loss 29.9123 (29.6397)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6520	Speed: 181.84 samples/s	Training Loss 29.8275 (29.7500)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6530	Speed: 182.09 samples/s	Training Loss 28.6963 (29.2763)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6540	Speed: 182.12 samples/s	Training Loss 30.1896 (29.7498)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6550	Speed: 181.83 samples/s	Training Loss 29.8880 (29.8732)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6560	Speed: 181.94 samples/s	Training Loss 30.0346 (29.7596)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6570	Speed: 181.97 samples/s	Training Loss 29.1640 (29.7006)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6580	Speed: 182.43 samples/s	Training Loss 29.9031 (29.8079)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6590	Speed: 182.11 samples/s	Training Loss 29.4731 (29.7394)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6600	Speed: 182.27 samples/s	Training Loss 30.5784 (29.8153)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][6600]XNorm: 39.42083
[lfw][6600]Accuracy-Flip: 0.76467+-0.01943
[lfw][6600]Best-Threshold: 0.95100
highest_acc: [0.7858333333333334]
Epoch 1 Batch 6610	Speed: 12.36 samples/s	Training Loss 29.6242 (29.6934)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6620	Speed: 181.87 samples/s	Training Loss 29.7271 (29.5751)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6630	Speed: 181.57 samples/s	Training Loss 29.7491 (29.8248)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6640	Speed: 181.65 samples/s	Training Loss 29.8183 (29.6616)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6650	Speed: 182.17 samples/s	Training Loss 29.3763 (29.5812)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6660	Speed: 181.58 samples/s	Training Loss 29.7328 (29.6959)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6670	Speed: 181.49 samples/s	Training Loss 29.8547 (29.5667)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6680	Speed: 181.78 samples/s	Training Loss 29.5547 (29.5614)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6690	Speed: 181.74 samples/s	Training Loss 30.1714 (29.3966)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6700	Speed: 181.71 samples/s	Training Loss 29.4927 (29.5934)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][6700]XNorm: 52.55628
[lfw][6700]Accuracy-Flip: 0.77300+-0.01806
[lfw][6700]Best-Threshold: 0.87800
highest_acc: [0.7858333333333334]
Epoch 1 Batch 6710	Speed: 12.28 samples/s	Training Loss 29.2765 (29.7097)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6720	Speed: 181.68 samples/s	Training Loss 29.7795 (29.9275)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6730	Speed: 181.10 samples/s	Training Loss 30.3072 (29.7726)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6740	Speed: 181.37 samples/s	Training Loss 30.1107 (29.5984)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6750	Speed: 181.19 samples/s	Training Loss 29.4147 (29.6356)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6760	Speed: 181.41 samples/s	Training Loss 29.9319 (29.6541)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6770	Speed: 181.13 samples/s	Training Loss 29.8614 (29.6838)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6780	Speed: 181.12 samples/s	Training Loss 30.1153 (29.6311)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6790	Speed: 181.38 samples/s	Training Loss 29.9669 (29.6055)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6800	Speed: 181.71 samples/s	Training Loss 29.7028 (29.7707)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][6800]XNorm: 93.19777
[lfw][6800]Accuracy-Flip: 0.77583+-0.01998
[lfw][6800]Best-Threshold: 0.91200
highest_acc: [0.7858333333333334]
Epoch 1 Batch 6810	Speed: 12.30 samples/s	Training Loss 29.0150 (29.5201)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6820	Speed: 181.42 samples/s	Training Loss 29.2323 (29.6526)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6830	Speed: 181.57 samples/s	Training Loss 29.0474 (29.4874)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6840	Speed: 181.30 samples/s	Training Loss 29.0612 (29.5467)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6850	Speed: 181.30 samples/s	Training Loss 29.5061 (29.6434)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6860	Speed: 181.26 samples/s	Training Loss 29.2563 (29.4301)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6870	Speed: 181.55 samples/s	Training Loss 30.1605 (29.6172)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6880	Speed: 181.42 samples/s	Training Loss 28.8226 (29.7593)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6890	Speed: 181.37 samples/s	Training Loss 29.3753 (29.5690)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6900	Speed: 181.37 samples/s	Training Loss 29.2016 (29.5937)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][6900]XNorm: 52.54333
[lfw][6900]Accuracy-Flip: 0.77967+-0.02141
[lfw][6900]Best-Threshold: 0.96200
highest_acc: [0.7858333333333334]
Epoch 1 Batch 6910	Speed: 12.10 samples/s	Training Loss 29.7503 (29.5423)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6920	Speed: 181.63 samples/s	Training Loss 29.7029 (29.7871)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6930	Speed: 181.33 samples/s	Training Loss 28.8014 (29.5326)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6940	Speed: 181.43 samples/s	Training Loss 29.6131 (29.6449)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6950	Speed: 181.64 samples/s	Training Loss 29.4457 (29.4030)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6960	Speed: 181.31 samples/s	Training Loss 29.3804 (29.5137)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6970	Speed: 181.58 samples/s	Training Loss 30.0224 (29.6147)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6980	Speed: 181.52 samples/s	Training Loss 29.3346 (29.6298)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 6990	Speed: 181.71 samples/s	Training Loss 28.5363 (29.5460)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7000	Speed: 181.84 samples/s	Training Loss 30.0232 (29.5548)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][7000]XNorm: 93.52133
[lfw][7000]Accuracy-Flip: 0.77200+-0.01447
[lfw][7000]Best-Threshold: 0.90800
highest_acc: [0.7858333333333334]
Epoch 1 Batch 7010	Speed: 12.31 samples/s	Training Loss 29.8297 (29.9026)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7020	Speed: 181.20 samples/s	Training Loss 29.9265 (29.4914)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7030	Speed: 181.24 samples/s	Training Loss 29.5281 (29.4486)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7040	Speed: 181.23 samples/s	Training Loss 29.3724 (29.7126)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7050	Speed: 180.79 samples/s	Training Loss 29.3520 (29.5546)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7060	Speed: 181.44 samples/s	Training Loss 29.9980 (29.6855)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7070	Speed: 180.93 samples/s	Training Loss 28.9186 (29.4895)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7080	Speed: 181.07 samples/s	Training Loss 29.4569 (29.4248)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7090	Speed: 181.06 samples/s	Training Loss 29.4681 (29.5570)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7100	Speed: 181.08 samples/s	Training Loss 29.7346 (29.5901)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][7100]XNorm: 58.89761
[lfw][7100]Accuracy-Flip: 0.76683+-0.01233
[lfw][7100]Best-Threshold: 0.89200
highest_acc: [0.7858333333333334]
Epoch 1 Batch 7110	Speed: 12.26 samples/s	Training Loss 30.6416 (29.4351)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7120	Speed: 180.82 samples/s	Training Loss 29.5784 (29.4412)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7130	Speed: 181.16 samples/s	Training Loss 29.6633 (29.8377)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7140	Speed: 180.72 samples/s	Training Loss 29.3887 (29.7131)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7150	Speed: 180.81 samples/s	Training Loss 29.6065 (29.5857)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7160	Speed: 180.88 samples/s	Training Loss 29.8975 (29.4547)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7170	Speed: 180.95 samples/s	Training Loss 28.5758 (29.7095)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7180	Speed: 181.04 samples/s	Training Loss 29.3942 (29.7007)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7190	Speed: 181.07 samples/s	Training Loss 29.5357 (29.6055)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7200	Speed: 181.20 samples/s	Training Loss 29.5429 (29.5774)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][7200]XNorm: 73.64128
[lfw][7200]Accuracy-Flip: 0.79533+-0.02005
[lfw][7200]Best-Threshold: 0.92500
highest_acc: [0.7953333333333334]
Epoch 1 Batch 7210	Speed: 12.25 samples/s	Training Loss 29.8244 (29.5241)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7220	Speed: 180.92 samples/s	Training Loss 29.7048 (29.5527)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7230	Speed: 180.78 samples/s	Training Loss 29.6962 (29.5944)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7240	Speed: 181.04 samples/s	Training Loss 30.1418 (29.4793)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7250	Speed: 180.93 samples/s	Training Loss 29.5967 (29.6218)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7260	Speed: 180.66 samples/s	Training Loss 30.1236 (29.5641)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7270	Speed: 180.95 samples/s	Training Loss 28.9990 (29.4488)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7280	Speed: 180.94 samples/s	Training Loss 29.3436 (29.5138)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7290	Speed: 180.97 samples/s	Training Loss 29.1037 (29.2184)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7300	Speed: 180.81 samples/s	Training Loss 28.9950 (29.5706)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][7300]XNorm: 18.18782
[lfw][7300]Accuracy-Flip: 0.79967+-0.01624
[lfw][7300]Best-Threshold: 0.93200
highest_acc: [0.7996666666666666]
Epoch 1 Batch 7310	Speed: 12.24 samples/s	Training Loss 29.2993 (29.5394)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7320	Speed: 180.99 samples/s	Training Loss 29.1825 (29.3759)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7330	Speed: 180.50 samples/s	Training Loss 29.3227 (29.4856)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7340	Speed: 180.70 samples/s	Training Loss 29.4389 (29.5218)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7350	Speed: 181.03 samples/s	Training Loss 29.6282 (29.4864)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7360	Speed: 180.74 samples/s	Training Loss 29.2583 (29.5038)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7370	Speed: 181.32 samples/s	Training Loss 29.2136 (29.3859)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7380	Speed: 181.26 samples/s	Training Loss 29.3781 (29.5119)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7390	Speed: 180.84 samples/s	Training Loss 29.2338 (29.5010)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7400	Speed: 180.96 samples/s	Training Loss 29.5334 (29.3973)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][7400]XNorm: 13.13083
[lfw][7400]Accuracy-Flip: 0.79767+-0.01506
[lfw][7400]Best-Threshold: 0.76200
highest_acc: [0.7996666666666666]
Epoch 1 Batch 7410	Speed: 12.24 samples/s	Training Loss 30.0543 (29.5976)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7420	Speed: 180.66 samples/s	Training Loss 28.9693 (29.4666)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7430	Speed: 180.67 samples/s	Training Loss 29.8335 (29.5357)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7440	Speed: 180.57 samples/s	Training Loss 29.0502 (29.4806)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7450	Speed: 180.73 samples/s	Training Loss 29.6509 (29.6130)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7460	Speed: 180.70 samples/s	Training Loss 28.6984 (29.2589)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7470	Speed: 180.74 samples/s	Training Loss 29.0257 (29.3556)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7480	Speed: 180.88 samples/s	Training Loss 29.7988 (29.4350)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7490	Speed: 180.79 samples/s	Training Loss 29.3669 (29.4785)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7500	Speed: 180.90 samples/s	Training Loss 29.4794 (29.2631)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][7500]XNorm: 4.29630
[lfw][7500]Accuracy-Flip: 0.80367+-0.01730
[lfw][7500]Best-Threshold: 0.83700
highest_acc: [0.8036666666666668]
Epoch 1 Batch 7510	Speed: 12.26 samples/s	Training Loss 29.8079 (29.3450)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7520	Speed: 181.17 samples/s	Training Loss 29.3134 (29.2044)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7530	Speed: 181.09 samples/s	Training Loss 29.8504 (29.4921)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7540	Speed: 180.84 samples/s	Training Loss 29.6859 (29.4979)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7550	Speed: 180.91 samples/s	Training Loss 29.5433 (29.5174)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7560	Speed: 181.17 samples/s	Training Loss 29.2752 (29.5371)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7570	Speed: 181.21 samples/s	Training Loss 29.5273 (29.3529)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7580	Speed: 180.73 samples/s	Training Loss 29.2427 (29.2501)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7590	Speed: 181.15 samples/s	Training Loss 28.9806 (29.5855)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7600	Speed: 181.03 samples/s	Training Loss 29.1115 (29.4494)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][7600]XNorm: 15.63804
[lfw][7600]Accuracy-Flip: 0.79833+-0.01716
[lfw][7600]Best-Threshold: 0.87600
highest_acc: [0.8036666666666668]
Epoch 1 Batch 7610	Speed: 12.25 samples/s	Training Loss 29.7674 (29.4162)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7620	Speed: 180.84 samples/s	Training Loss 29.5155 (29.6202)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7630	Speed: 181.04 samples/s	Training Loss 29.3742 (29.5174)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7640	Speed: 180.89 samples/s	Training Loss 29.1426 (29.1392)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7650	Speed: 180.93 samples/s	Training Loss 29.1795 (29.4899)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7660	Speed: 180.96 samples/s	Training Loss 29.3225 (29.5445)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7670	Speed: 180.87 samples/s	Training Loss 29.8555 (29.3817)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7680	Speed: 180.80 samples/s	Training Loss 29.9448 (29.4747)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7690	Speed: 180.90 samples/s	Training Loss 29.3025 (29.2820)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7700	Speed: 181.07 samples/s	Training Loss 28.8402 (29.3721)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][7700]XNorm: 4.09976
[lfw][7700]Accuracy-Flip: 0.79433+-0.01583
[lfw][7700]Best-Threshold: 0.96300
highest_acc: [0.8036666666666668]
Epoch 1 Batch 7710	Speed: 12.18 samples/s	Training Loss 29.1771 (29.5141)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7720	Speed: 181.37 samples/s	Training Loss 29.5843 (29.3220)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7730	Speed: 181.40 samples/s	Training Loss 30.1729 (29.3882)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7740	Speed: 181.41 samples/s	Training Loss 29.7427 (29.5924)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7750	Speed: 181.43 samples/s	Training Loss 29.3358 (29.2689)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7760	Speed: 181.27 samples/s	Training Loss 29.0433 (29.2520)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7770	Speed: 181.09 samples/s	Training Loss 29.1043 (29.2433)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7780	Speed: 181.13 samples/s	Training Loss 29.5387 (29.1409)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7790	Speed: 181.26 samples/s	Training Loss 29.6322 (29.3624)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7800	Speed: 181.29 samples/s	Training Loss 29.2428 (29.3738)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][7800]XNorm: 16.10665
[lfw][7800]Accuracy-Flip: 0.80133+-0.01881
[lfw][7800]Best-Threshold: 0.86800
highest_acc: [0.8036666666666668]
Epoch 1 Batch 7810	Speed: 12.02 samples/s	Training Loss 29.0688 (29.1436)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7820	Speed: 181.73 samples/s	Training Loss 29.2230 (29.2620)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7830	Speed: 181.90 samples/s	Training Loss 29.0927 (29.1318)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7840	Speed: 181.67 samples/s	Training Loss 29.1776 (29.3140)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7850	Speed: 181.71 samples/s	Training Loss 28.9659 (29.1216)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7860	Speed: 181.77 samples/s	Training Loss 28.9121 (29.1580)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7870	Speed: 181.66 samples/s	Training Loss 29.4022 (29.3073)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7880	Speed: 181.69 samples/s	Training Loss 28.9293 (29.2776)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7890	Speed: 181.49 samples/s	Training Loss 28.3451 (29.4107)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7900	Speed: 181.57 samples/s	Training Loss 29.6098 (29.3268)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][7900]XNorm: 7.11698
[lfw][7900]Accuracy-Flip: 0.80967+-0.01929
[lfw][7900]Best-Threshold: 0.84300
highest_acc: [0.8096666666666668]
Epoch 1 Batch 7910	Speed: 12.32 samples/s	Training Loss 28.8809 (29.1247)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7920	Speed: 181.97 samples/s	Training Loss 29.2638 (29.3675)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7930	Speed: 181.91 samples/s	Training Loss 28.6370 (29.1932)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7940	Speed: 182.07 samples/s	Training Loss 29.3379 (29.1895)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7950	Speed: 181.78 samples/s	Training Loss 29.2322 (29.2113)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7960	Speed: 182.00 samples/s	Training Loss 29.5720 (29.3209)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7970	Speed: 181.91 samples/s	Training Loss 28.5222 (28.9088)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7980	Speed: 182.05 samples/s	Training Loss 28.7945 (29.2472)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 7990	Speed: 181.93 samples/s	Training Loss 29.0136 (29.1613)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8000	Speed: 181.77 samples/s	Training Loss 29.2774 (29.2049)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][8000]XNorm: 40.90885
[lfw][8000]Accuracy-Flip: 0.80533+-0.01551
[lfw][8000]Best-Threshold: 0.91400
highest_acc: [0.8096666666666668]
Epoch 1 Batch 8010	Speed: 12.30 samples/s	Training Loss 28.7930 (29.3637)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8020	Speed: 181.45 samples/s	Training Loss 29.8358 (29.2730)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8030	Speed: 181.17 samples/s	Training Loss 28.8418 (29.1447)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8040	Speed: 181.21 samples/s	Training Loss 29.7912 (29.3944)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8050	Speed: 181.24 samples/s	Training Loss 29.9878 (29.5137)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8060	Speed: 181.18 samples/s	Training Loss 29.8112 (29.5965)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8070	Speed: 181.15 samples/s	Training Loss 29.4958 (29.1922)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8080	Speed: 181.13 samples/s	Training Loss 30.0412 (29.2900)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8090	Speed: 181.13 samples/s	Training Loss 29.7613 (29.2705)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8100	Speed: 181.43 samples/s	Training Loss 28.9434 (29.3001)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][8100]XNorm: 33.73872
[lfw][8100]Accuracy-Flip: 0.80483+-0.01947
[lfw][8100]Best-Threshold: 0.92000
highest_acc: [0.8096666666666668]
Epoch 1 Batch 8110	Speed: 12.25 samples/s	Training Loss 28.9518 (29.3047)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8120	Speed: 181.59 samples/s	Training Loss 29.6160 (29.4465)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8130	Speed: 181.56 samples/s	Training Loss 29.5777 (29.0284)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8140	Speed: 181.60 samples/s	Training Loss 29.2524 (29.1270)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8150	Speed: 181.48 samples/s	Training Loss 29.4672 (29.3353)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8160	Speed: 181.54 samples/s	Training Loss 29.1459 (28.9953)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8170	Speed: 181.56 samples/s	Training Loss 29.5392 (29.4594)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8180	Speed: 181.31 samples/s	Training Loss 30.5524 (29.9347)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8190	Speed: 181.29 samples/s	Training Loss 29.7387 (29.9444)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8200	Speed: 181.44 samples/s	Training Loss 29.9750 (29.7251)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][8200]XNorm: 5.38598
[lfw][8200]Accuracy-Flip: 0.77517+-0.01816
[lfw][8200]Best-Threshold: 0.83500
highest_acc: [0.8096666666666668]
Epoch 1 Batch 8210	Speed: 12.27 samples/s	Training Loss 29.3759 (29.7009)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8220	Speed: 181.18 samples/s	Training Loss 29.4734 (29.4556)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8230	Speed: 180.99 samples/s	Training Loss 29.6462 (29.4926)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8240	Speed: 180.84 samples/s	Training Loss 29.8595 (29.7763)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8250	Speed: 180.91 samples/s	Training Loss 29.7120 (29.2930)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8260	Speed: 181.04 samples/s	Training Loss 28.6607 (29.3372)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8270	Speed: 180.96 samples/s	Training Loss 28.7416 (29.4266)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8280	Speed: 180.98 samples/s	Training Loss 29.9594 (29.3296)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8290	Speed: 180.82 samples/s	Training Loss 29.1336 (29.2830)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8300	Speed: 180.86 samples/s	Training Loss 29.9538 (29.5106)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][8300]XNorm: 11.01468
[lfw][8300]Accuracy-Flip: 0.79400+-0.01336
[lfw][8300]Best-Threshold: 0.85300
highest_acc: [0.8096666666666668]
Epoch 1 Batch 8310	Speed: 12.23 samples/s	Training Loss 28.8706 (29.2688)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8320	Speed: 181.10 samples/s	Training Loss 28.8828 (29.1016)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8330	Speed: 181.06 samples/s	Training Loss 29.1034 (29.2675)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8340	Speed: 180.98 samples/s	Training Loss 30.0995 (29.3934)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8350	Speed: 181.06 samples/s	Training Loss 29.7972 (29.5618)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8360	Speed: 181.00 samples/s	Training Loss 29.6841 (29.8060)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8370	Speed: 181.22 samples/s	Training Loss 28.9403 (29.5209)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8380	Speed: 180.32 samples/s	Training Loss 29.6405 (29.3606)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8390	Speed: 180.87 samples/s	Training Loss 29.6327 (29.4270)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8400	Speed: 180.89 samples/s	Training Loss 29.5800 (29.5557)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][8400]XNorm: 4.72647
[lfw][8400]Accuracy-Flip: 0.79033+-0.01890
[lfw][8400]Best-Threshold: 0.87000
highest_acc: [0.8096666666666668]
Epoch 1 Batch 8410	Speed: 12.20 samples/s	Training Loss 29.2084 (29.4213)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8420	Speed: 180.32 samples/s	Training Loss 28.9879 (29.2353)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8430	Speed: 180.22 samples/s	Training Loss 29.8384 (29.2036)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8440	Speed: 180.40 samples/s	Training Loss 29.2462 (29.5802)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8450	Speed: 180.47 samples/s	Training Loss 29.5317 (29.5194)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8460	Speed: 180.37 samples/s	Training Loss 29.4226 (29.3113)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8470	Speed: 180.31 samples/s	Training Loss 29.2257 (29.3123)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8480	Speed: 180.53 samples/s	Training Loss 29.1726 (29.3115)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8490	Speed: 180.45 samples/s	Training Loss 29.6242 (29.3411)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8500	Speed: 180.45 samples/s	Training Loss 29.1679 (29.3166)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][8500]XNorm: 50.85945
[lfw][8500]Accuracy-Flip: 0.76300+-0.01821
[lfw][8500]Best-Threshold: 0.85500
highest_acc: [0.8096666666666668]
Epoch 1 Batch 8510	Speed: 12.22 samples/s	Training Loss 28.9154 (29.0519)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8520	Speed: 181.04 samples/s	Training Loss 29.6723 (29.1050)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8530	Speed: 180.86 samples/s	Training Loss 28.9350 (29.3541)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8540	Speed: 181.03 samples/s	Training Loss 29.0358 (29.1244)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8550	Speed: 180.81 samples/s	Training Loss 28.9466 (29.2340)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8560	Speed: 180.97 samples/s	Training Loss 28.9873 (29.2498)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8570	Speed: 180.73 samples/s	Training Loss 29.3758 (29.1027)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8580	Speed: 180.84 samples/s	Training Loss 29.1230 (29.2313)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8590	Speed: 180.76 samples/s	Training Loss 28.7600 (28.9237)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8600	Speed: 180.75 samples/s	Training Loss 28.6364 (28.9398)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][8600]XNorm: 4.21072
[lfw][8600]Accuracy-Flip: 0.80500+-0.01498
[lfw][8600]Best-Threshold: 0.83400
highest_acc: [0.8096666666666668]
Epoch 1 Batch 8610	Speed: 12.24 samples/s	Training Loss 28.9538 (29.2810)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8620	Speed: 181.02 samples/s	Training Loss 29.1928 (29.1767)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8630	Speed: 181.01 samples/s	Training Loss 29.4407 (29.0142)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8640	Speed: 181.08 samples/s	Training Loss 28.5620 (29.2360)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8650	Speed: 180.93 samples/s	Training Loss 28.9469 (28.9903)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8660	Speed: 180.98 samples/s	Training Loss 29.6283 (29.1663)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8670	Speed: 180.96 samples/s	Training Loss 28.5660 (29.1170)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8680	Speed: 181.05 samples/s	Training Loss 28.7938 (28.9998)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8690	Speed: 181.01 samples/s	Training Loss 29.5411 (29.0436)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8700	Speed: 181.07 samples/s	Training Loss 28.4734 (28.9875)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][8700]XNorm: 4.63717
[lfw][8700]Accuracy-Flip: 0.81333+-0.01462
[lfw][8700]Best-Threshold: 0.94700
highest_acc: [0.8133333333333332]
Epoch 1 Batch 8710	Speed: 12.09 samples/s	Training Loss 28.7013 (29.1146)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8720	Speed: 181.19 samples/s	Training Loss 28.9829 (29.2607)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8730	Speed: 181.28 samples/s	Training Loss 28.7303 (29.1349)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8740	Speed: 181.28 samples/s	Training Loss 27.9018 (29.2097)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8750	Speed: 181.15 samples/s	Training Loss 28.9397 (29.0918)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8760	Speed: 181.12 samples/s	Training Loss 28.5956 (29.1027)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8770	Speed: 181.04 samples/s	Training Loss 29.2091 (29.1839)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8780	Speed: 181.04 samples/s	Training Loss 29.9891 (29.2635)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8790	Speed: 181.03 samples/s	Training Loss 29.2331 (29.1393)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8800	Speed: 181.12 samples/s	Training Loss 29.7291 (29.2657)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][8800]XNorm: 5.41703
[lfw][8800]Accuracy-Flip: 0.79167+-0.01999
[lfw][8800]Best-Threshold: 0.97400
highest_acc: [0.8133333333333332]
Epoch 1 Batch 8810	Speed: 12.01 samples/s	Training Loss 28.7496 (29.0200)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8820	Speed: 181.14 samples/s	Training Loss 29.3544 (29.1425)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8830	Speed: 181.06 samples/s	Training Loss 29.2508 (29.1307)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8840	Speed: 181.10 samples/s	Training Loss 29.4524 (29.0750)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8850	Speed: 180.99 samples/s	Training Loss 29.1981 (29.1845)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8860	Speed: 181.15 samples/s	Training Loss 29.4511 (29.1287)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8870	Speed: 181.06 samples/s	Training Loss 29.1250 (29.0194)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8880	Speed: 181.07 samples/s	Training Loss 28.7066 (29.0690)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8890	Speed: 181.16 samples/s	Training Loss 29.0622 (28.8760)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8900	Speed: 181.19 samples/s	Training Loss 29.0955 (29.0614)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][8900]XNorm: 7.23301
[lfw][8900]Accuracy-Flip: 0.80317+-0.01947
[lfw][8900]Best-Threshold: 0.86100
highest_acc: [0.8133333333333332]
Epoch 1 Batch 8910	Speed: 12.21 samples/s	Training Loss 28.9971 (29.0868)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8920	Speed: 180.99 samples/s	Training Loss 28.5314 (28.9752)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8930	Speed: 181.04 samples/s	Training Loss 28.8793 (28.9546)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8940	Speed: 180.94 samples/s	Training Loss 29.3320 (29.0861)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8950	Speed: 180.73 samples/s	Training Loss 28.4920 (29.1423)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8960	Speed: 181.26 samples/s	Training Loss 29.3951 (29.1777)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8970	Speed: 181.08 samples/s	Training Loss 29.1518 (29.0956)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8980	Speed: 181.13 samples/s	Training Loss 28.6398 (28.9197)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 8990	Speed: 181.02 samples/s	Training Loss 28.1707 (28.8980)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9000	Speed: 181.17 samples/s	Training Loss 29.2448 (29.0923)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][9000]XNorm: 4.98434
[lfw][9000]Accuracy-Flip: 0.81000+-0.01693
[lfw][9000]Best-Threshold: 0.94400
highest_acc: [0.8133333333333332]
Epoch 1 Batch 9010	Speed: 12.26 samples/s	Training Loss 29.1416 (29.1419)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9020	Speed: 180.97 samples/s	Training Loss 28.4109 (28.8173)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9030	Speed: 181.24 samples/s	Training Loss 29.1742 (28.8973)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9040	Speed: 181.15 samples/s	Training Loss 28.8813 (29.1472)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9050	Speed: 181.22 samples/s	Training Loss 27.8866 (29.1385)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9060	Speed: 181.26 samples/s	Training Loss 29.1544 (29.1018)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9070	Speed: 181.23 samples/s	Training Loss 29.3750 (29.0143)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9080	Speed: 181.04 samples/s	Training Loss 28.5595 (29.0704)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9090	Speed: 181.34 samples/s	Training Loss 28.8121 (28.8120)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9100	Speed: 181.21 samples/s	Training Loss 29.5083 (28.8140)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][9100]XNorm: 4.12542
[lfw][9100]Accuracy-Flip: 0.80883+-0.02670
[lfw][9100]Best-Threshold: 0.80500
highest_acc: [0.8133333333333332]
Epoch 1 Batch 9110	Speed: 12.25 samples/s	Training Loss 29.5753 (28.7684)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9120	Speed: 181.04 samples/s	Training Loss 28.8201 (28.8300)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9130	Speed: 181.12 samples/s	Training Loss 28.8092 (29.2540)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9140	Speed: 181.38 samples/s	Training Loss 28.7428 (28.7875)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9150	Speed: 181.21 samples/s	Training Loss 28.2343 (28.9555)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9160	Speed: 181.20 samples/s	Training Loss 29.1165 (28.8593)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9170	Speed: 181.32 samples/s	Training Loss 29.1493 (28.7571)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9180	Speed: 181.20 samples/s	Training Loss 27.9528 (28.6722)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9190	Speed: 181.32 samples/s	Training Loss 29.0529 (29.1601)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9200	Speed: 181.14 samples/s	Training Loss 29.2076 (28.8272)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][9200]XNorm: 7.64612
[lfw][9200]Accuracy-Flip: 0.80950+-0.01962
[lfw][9200]Best-Threshold: 0.84800
highest_acc: [0.8133333333333332]
Epoch 1 Batch 9210	Speed: 12.27 samples/s	Training Loss 28.8882 (28.9736)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9220	Speed: 181.23 samples/s	Training Loss 28.4403 (28.6344)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9230	Speed: 181.48 samples/s	Training Loss 29.0425 (28.9471)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9240	Speed: 181.32 samples/s	Training Loss 28.5105 (29.0306)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9250	Speed: 181.33 samples/s	Training Loss 28.3739 (28.8456)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9260	Speed: 181.00 samples/s	Training Loss 29.1289 (28.9533)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9270	Speed: 181.11 samples/s	Training Loss 29.4587 (28.8960)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9280	Speed: 181.05 samples/s	Training Loss 28.7555 (28.7889)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9290	Speed: 181.27 samples/s	Training Loss 29.0186 (28.9324)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9300	Speed: 181.18 samples/s	Training Loss 28.6220 (28.9446)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][9300]XNorm: 7.38649
[lfw][9300]Accuracy-Flip: 0.81517+-0.01950
[lfw][9300]Best-Threshold: 0.80800
highest_acc: [0.8151666666666667]
Epoch 1 Batch 9310	Speed: 12.24 samples/s	Training Loss 29.1742 (28.8207)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9320	Speed: 181.10 samples/s	Training Loss 29.0321 (28.7811)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9330	Speed: 181.22 samples/s	Training Loss 28.1012 (28.5255)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9340	Speed: 181.23 samples/s	Training Loss 28.3849 (28.8391)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9350	Speed: 181.16 samples/s	Training Loss 28.5835 (28.7083)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9360	Speed: 181.19 samples/s	Training Loss 28.7433 (28.8344)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9370	Speed: 181.00 samples/s	Training Loss 28.9731 (28.8923)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9380	Speed: 181.00 samples/s	Training Loss 28.0308 (28.8199)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9390	Speed: 181.13 samples/s	Training Loss 29.5557 (28.5942)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9400	Speed: 181.14 samples/s	Training Loss 28.4801 (28.7469)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][9400]XNorm: 7.28239
[lfw][9400]Accuracy-Flip: 0.80733+-0.02025
[lfw][9400]Best-Threshold: 0.88400
highest_acc: [0.8151666666666667]
Epoch 1 Batch 9410	Speed: 12.25 samples/s	Training Loss 28.6245 (28.7968)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9420	Speed: 181.30 samples/s	Training Loss 28.9809 (28.7294)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9430	Speed: 181.23 samples/s	Training Loss 29.0229 (28.7007)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9440	Speed: 181.04 samples/s	Training Loss 27.7785 (28.6222)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9450	Speed: 181.35 samples/s	Training Loss 28.6432 (28.9914)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9460	Speed: 181.03 samples/s	Training Loss 29.1379 (28.4221)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9470	Speed: 181.13 samples/s	Training Loss 29.2346 (28.7786)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9480	Speed: 181.25 samples/s	Training Loss 28.4822 (28.7704)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9490	Speed: 181.39 samples/s	Training Loss 27.6798 (28.7174)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9500	Speed: 181.19 samples/s	Training Loss 27.9883 (28.8666)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][9500]XNorm: 4.38149
[lfw][9500]Accuracy-Flip: 0.81400+-0.02195
[lfw][9500]Best-Threshold: 0.91300
highest_acc: [0.8151666666666667]
Epoch 1 Batch 9510	Speed: 12.25 samples/s	Training Loss 29.8566 (28.8337)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9520	Speed: 181.09 samples/s	Training Loss 28.1782 (28.8126)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9530	Speed: 181.18 samples/s	Training Loss 28.6008 (28.6354)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9540	Speed: 181.06 samples/s	Training Loss 28.3205 (28.6374)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9550	Speed: 181.29 samples/s	Training Loss 29.0698 (28.7408)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9560	Speed: 181.04 samples/s	Training Loss 28.6471 (28.9548)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9570	Speed: 181.19 samples/s	Training Loss 28.8593 (28.5179)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9580	Speed: 181.12 samples/s	Training Loss 28.7358 (28.5872)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9590	Speed: 181.20 samples/s	Training Loss 28.3769 (28.6373)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9600	Speed: 181.17 samples/s	Training Loss 29.1251 (28.7755)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][9600]XNorm: 6.74205
[lfw][9600]Accuracy-Flip: 0.80600+-0.02218
[lfw][9600]Best-Threshold: 0.93100
highest_acc: [0.8151666666666667]
Epoch 1 Batch 9610	Speed: 12.25 samples/s	Training Loss 29.2129 (28.5167)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9620	Speed: 181.02 samples/s	Training Loss 28.7414 (28.7008)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9630	Speed: 181.00 samples/s	Training Loss 28.6380 (28.6424)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9640	Speed: 180.93 samples/s	Training Loss 27.5973 (28.4162)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9650	Speed: 180.96 samples/s	Training Loss 28.7545 (28.7427)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9660	Speed: 180.94 samples/s	Training Loss 28.4664 (28.7547)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9670	Speed: 180.97 samples/s	Training Loss 28.7356 (28.5744)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9680	Speed: 180.96 samples/s	Training Loss 28.7167 (28.6182)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9690	Speed: 180.95 samples/s	Training Loss 28.9292 (28.5159)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9700	Speed: 180.94 samples/s	Training Loss 28.7583 (28.5843)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][9700]XNorm: 4.82178
[lfw][9700]Accuracy-Flip: 0.81283+-0.01975
[lfw][9700]Best-Threshold: 0.87300
highest_acc: [0.8151666666666667]
Epoch 1 Batch 9710	Speed: 12.15 samples/s	Training Loss 27.7551 (28.5215)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9720	Speed: 180.77 samples/s	Training Loss 28.4768 (28.7061)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9730	Speed: 180.10 samples/s	Training Loss 29.2607 (28.6912)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9740	Speed: 180.92 samples/s	Training Loss 28.6309 (28.6079)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9750	Speed: 180.45 samples/s	Training Loss 29.0377 (28.7686)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9760	Speed: 180.69 samples/s	Training Loss 28.8321 (28.7079)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9770	Speed: 180.50 samples/s	Training Loss 28.2992 (28.6735)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9780	Speed: 180.54 samples/s	Training Loss 28.9502 (28.6741)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9790	Speed: 180.63 samples/s	Training Loss 28.3065 (28.6150)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9800	Speed: 180.87 samples/s	Training Loss 28.8800 (28.6573)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][9800]XNorm: 6.79469
[lfw][9800]Accuracy-Flip: 0.81867+-0.01866
[lfw][9800]Best-Threshold: 0.87200
highest_acc: [0.8186666666666668]
Epoch 1 Batch 9810	Speed: 11.85 samples/s	Training Loss 28.4001 (28.4757)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9820	Speed: 181.23 samples/s	Training Loss 28.6419 (28.3045)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9830	Speed: 181.06 samples/s	Training Loss 27.9742 (28.6050)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9840	Speed: 180.88 samples/s	Training Loss 28.0900 (28.4887)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9850	Speed: 180.82 samples/s	Training Loss 28.4645 (28.5835)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9860	Speed: 181.03 samples/s	Training Loss 29.1866 (28.6903)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9870	Speed: 180.95 samples/s	Training Loss 28.9991 (28.6663)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9880	Speed: 180.81 samples/s	Training Loss 28.2315 (28.5046)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9890	Speed: 180.63 samples/s	Training Loss 29.1342 (29.2806)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9900	Speed: 180.66 samples/s	Training Loss 29.2882 (28.9806)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][9900]XNorm: 19.08157
[lfw][9900]Accuracy-Flip: 0.75583+-0.01265
[lfw][9900]Best-Threshold: 0.73400
highest_acc: [0.8186666666666668]
Epoch 1 Batch 9910	Speed: 12.24 samples/s	Training Loss 28.9938 (28.7903)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9920	Speed: 180.96 samples/s	Training Loss 29.7763 (28.6516)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9930	Speed: 180.93 samples/s	Training Loss 28.6020 (28.6795)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9940	Speed: 181.06 samples/s	Training Loss 28.0960 (28.5786)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9950	Speed: 180.97 samples/s	Training Loss 29.1198 (28.7380)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9960	Speed: 180.98 samples/s	Training Loss 27.9181 (28.6362)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9970	Speed: 181.00 samples/s	Training Loss 28.0894 (28.7053)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9980	Speed: 180.88 samples/s	Training Loss 28.1779 (28.7633)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 9990	Speed: 180.83 samples/s	Training Loss 29.2712 (28.6956)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10000	Speed: 180.97 samples/s	Training Loss 28.8489 (28.7871)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][10000]XNorm: 5.59189
[lfw][10000]Accuracy-Flip: 0.81500+-0.01696
[lfw][10000]Best-Threshold: 0.84900
highest_acc: [0.8186666666666668]
Epoch 1 Batch 10010	Speed: 12.25 samples/s	Training Loss 27.9647 (28.6143)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10020	Speed: 180.82 samples/s	Training Loss 28.6551 (28.7616)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10030	Speed: 180.74 samples/s	Training Loss 27.8025 (28.3782)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10040	Speed: 180.72 samples/s	Training Loss 28.8793 (28.6091)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10050	Speed: 180.99 samples/s	Training Loss 28.2796 (28.6605)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10060	Speed: 180.97 samples/s	Training Loss 28.7405 (28.6552)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10070	Speed: 181.00 samples/s	Training Loss 29.6994 (29.0456)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10080	Speed: 180.73 samples/s	Training Loss 28.4179 (28.8391)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10090	Speed: 180.78 samples/s	Training Loss 29.4116 (28.8156)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10100	Speed: 180.74 samples/s	Training Loss 29.3904 (28.7868)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][10100]XNorm: 779.83388
[lfw][10100]Accuracy-Flip: 0.78400+-0.02340
[lfw][10100]Best-Threshold: 0.97000
highest_acc: [0.8186666666666668]
Epoch 1 Batch 10110	Speed: 12.25 samples/s	Training Loss 28.3296 (28.7579)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10120	Speed: 180.98 samples/s	Training Loss 28.3866 (28.4684)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10130	Speed: 180.86 samples/s	Training Loss 28.8704 (28.7351)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10140	Speed: 180.93 samples/s	Training Loss 28.6382 (28.6010)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10150	Speed: 180.86 samples/s	Training Loss 28.6829 (28.4528)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10160	Speed: 180.83 samples/s	Training Loss 28.5468 (28.6947)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10170	Speed: 180.91 samples/s	Training Loss 28.2488 (28.4531)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10180	Speed: 180.68 samples/s	Training Loss 28.6750 (28.6073)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10190	Speed: 180.70 samples/s	Training Loss 28.5787 (28.5722)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10200	Speed: 180.75 samples/s	Training Loss 28.6875 (28.4826)	Training Prec@1 0.000 (0.000)
Learning rate 0.000500
Perform Evaluation on  ['lfw'] , and Save Checkpoints...
(12000, 512)
[lfw][10200]XNorm: 36.06209
[lfw][10200]Accuracy-Flip: 0.80267+-0.01821
[lfw][10200]Best-Threshold: 0.89800
highest_acc: [0.8186666666666668]
Epoch 1 Batch 10210	Speed: 12.24 samples/s	Training Loss 28.4494 (28.4307)	Training Prec@1 0.000 (0.000)
Epoch 1 Batch 10220	Speed: 181.11 samples/s	Training Loss 28.2290 (28.2622)	Training Prec@1 0.000 (0.000)